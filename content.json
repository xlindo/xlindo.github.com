{"posts":[{"title":"2013年，夏天的乐队吉他手","text":"https://xlindo.com所有文章遵循CC BY-NC-SA 4.0许可协议。请按许可使用。 因为没有我特别喜欢的乐队，所以还没集中找时间去看“乐队的夏天”，不过这倒是让我想起了那个夏天在乌鲁木齐的青旅遇见的那位乐队吉他手… 这位兄弟看着瘦瘦小小，人帅话不多（其实是不说话），二十来岁，留着长发，性冷淡风的着装仿佛注明着日本国籍。 从我入住的第一天起，他都一个人坐在休息区的大长桌的末头，戴着耳机，总是目不转睛地盯着笔记本屏幕。在青旅惯有的暗黑色系里，脸上快速闪烁的光硬是让他又多了几分冷酷和神秘。 由于那时候也是我旅行的末段，所以我也常待在休息区放空。自然地，我向他表示友好，“Are you a Japanese ？” 他向我微微一笑，“我看着很像日本人吗？” 我果断哈哈哈哈一阵笑，尽力处置尴尬。 可能是因为我是唯一一个愿意去和这个“日本”小哥打开话匣子的人，接下来的两三天里，我竟然化身成为了他的唯一陪聊对象，也豁然开朗了他这样酷酷的的原因。 他是一个地下乐队的主音吉他手（注：比另一种扒拉扒拉扫和弦的吉他手进阶一些），在一家国企上班，请假出来放松的。一个有意思的事是，这位兄弟的旅程安排就是坐飞机从贵阳出发，来到乌鲁木齐这家青旅，除去出去吃饭的时间，就是在这长桌边玩电脑、听音乐，如此几天，就结束旅程回贵阳了，并没有去别的地方的安排。 我也自认是一个脚趾踏进了摇滚圈了的人，所以自然想知道他是不是跟我吹牛呢。我找来青旅全国统一标准的，断了一根弦的市价200左右的吉他，递给他，“可惜没吉他给你秀一秀，这个能试试吗？” 没想到的是，小哥接过吉他，马上开启人耳调音模式，拧拧拨拨几个回合，耗时甚至少于擦灰，满意的微微一笑，“四根弦五根弦的吉他我没问题的。” 接下来就切换到 Live solo show 模式，引来至少十个人的应援。星光熠熠的感觉，让我恍惚中午坐我旁边吃着大碗宽面、磕着蒜瓣的和他是不是同一个人。 最让我印象深刻的还是有一天晚上，大概快1点了，我刷完手机后，准备上个厕所（公厕）就回来睡觉。路过二楼楼梯口的时候，瞟见他还在一楼休息区长桌那坐着，不巧的是，他居然也看见了我。他兴冲冲的蹿了过来，全然不顾我的睡意，把我拉到了他电脑旁边，“你来听听，我今天听到几首歌，很不错。” 我说，“你们搞摇滚的都不按时睡觉吗？” 可终究还是难逃。于是，夜深人静，孤男寡男，一人一只耳机，鬼鬼祟祟的在一片黑暗中听了半小时Rock ‘n’ Roll 。 后来他提出来并关注了我的微博，我也理所当然的回关了。我也因此看到他后来很多的演出现场视频，很摇滚、很地下，比我想象的要高级很多。陆陆续续的，也有了一些出国演出的视频。… 一直没聊天，也确实不知道能聊啥。… 再后来，我不再在微博好友圈里看到他的消息，才发现他已经取关我了，我也理所当然的取关了他。我想，也可能是因为他快红了吧，哈哈哈。祝他一切都好。 故事就讲到这儿了，这个故事其实是一个关于摇滚的故事。这时要问我什么是摇滚，我会说这位兄弟这种生活就很摇滚。 对了，最后一个小插曲是，我走的那天买了好几个大伽师瓜，他非要说要帮我拿到公交车站，我说这太沉了你拿不动，他坚持说最后一定要送送我。然而，他那青筋暴起的小胳膊终究还是只搬动了几米的距离。我笑笑接过来箱子，说，“好啦，再见啦！” 2013.8 - 2019.7","link":"/posts/dbe6178b/"},{"title":"2016年度总结","text":"https://xlindo.com所有文章遵循CC BY-NC-SA 4.0许可协议。请按许可使用。 2016过去了，2017是一个新的开始。 给自己的博客又搬了一次家，半截子文章先撂倒这个，全新的开始就从全新的Hexo开始吧。 一不小心又折腾了两个多小时没动过，我先睡觉了。","link":"/posts/def21828/"},{"title":"2017年元旦回了一次家","text":"一晃又是一年过去了，新的一年里又有很多的事等着去做。 因为上次考试报名没有报上，就又是近3个月没有见过父母了，这个自己既定的目标又相去甚远。 趁着这次元旦假期，恰巧妹子也有假，就在1月1号回家，在家待了一晚。 其实不管怎么说，每次回家一次，就能明显的感觉到家里的变化——父母以及置办，很担忧，但是能做的又不多，只能尽自己的努力去让自己的父母能像自己从前一样的感受到溺爱。 今年要做的事还很多，加油吧！","link":"/posts/b220a9ad/"},{"title":"C-Cpp-public_private_protected","text":"https://xlindo.com所有文章遵循CC BY-NC-SA 4.0许可协议。请按许可使用。 三段程序说明“类”相关的public、private、protected的概念 xlindo 2017年12月14日 原文：深入理解C++中public、protected及private用法 一般性概念 类的一个特征就是封装，public和private作用就是实现这一目的。所以：用户代码（类外）可以访问public成员而不能访问private成员；private成员只能由类成员（类内）和友元访问。 类的另一个特征就是继承，protected的作用就是实现这一目的。所以：protected成员可以被派生类对象访问，不能被用户代码（类外）访问。 12345678910111213141516171819202122232425262728293031323334353637383940#include &lt;assert.h&gt;#include &lt;iostream&gt;using namespace std;class A {public: int a; A() { a1 = 1; a2 = 2; a3 = 3; a = 4; } void fun() { cout &lt;&lt; a &lt;&lt; endl; //正确 cout &lt;&lt; a1 &lt;&lt; endl; //正确 cout &lt;&lt; a2 &lt;&lt; endl; //正确，类内访问 cout &lt;&lt; a3 &lt;&lt; endl; //正确，类内访问 }public: int a1;protected: int a2;private: int a3;};int main(){ A itema; itema.a = 10; //正确 itema.a1 = 20; //正确 itema.a2 = 30; //错误，类外不能访问protected成员 itema.a3 = 40; //错误，类外不能访问private成员 system(&quot;pause&quot;); return 0;} 三种继承另外，有public, protected, private三种继承方式，它们相应地改变了基类成员的访问属性： public继承：基类public成员，protected成员，private成员的访问属性在派生类中分别变成：public, protected, private protected继承：基类public成员，protected成员，private成员的访问属性在派生类中分别变成：protected, protected, private private继承：基类public成员，protected成员，private成员的访问属性在派生类中分别变成：private, private, private 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172#include &lt;iostream&gt;using namespace std;//////////////////////////////////////////////////////////////////////////class A //父类{private: int privatedateA;protected: int protecteddateA;public: int publicdateA;};//////////////////////////////////////////////////////////////////////////class B : public A //基类A的派生类B（共有继承）{public: void funct() { int b; b = privatedateA; //error：基类中私有成员在派生类中是不可见的 b = protecteddateA; //ok：基类的保护成员在派生类中为保护成员 b = publicdateA; //ok：基类的公共成员在派生类中为公共成员 }};//////////////////////////////////////////////////////////////////////////class C : private A //基类A的派生类C（私有继承）{public: void funct() { int c; c = privatedateA; //error：基类中私有成员在派生类中是不可见的 c = protecteddateA; //ok：基类的保护成员在派生类中为私有成员 c = publicdateA; //ok：基类的公共成员在派生类中为私有成员 }};//////////////////////////////////////////////////////////////////////////class D : protected A //基类A的派生类D（保护继承）{public: void funct() { int d; d = privatedateA; //error：基类中私有成员在派生类中是不可见的 d = protecteddateA; //ok：基类的保护成员在派生类中为保护成员 d = publicdateA; //ok：基类的公共成员在派生类中为保护成员 }};//////////////////////////////////////////////////////////////////////////int main(){ int a; B objB; a = objB.privatedateA; //error：基类中私有成员在派生类中是不可见的,对对象不可见 a = objB.protecteddateA; //error：基类的保护成员在派生类中为保护成员，对对象不可见 a = objB.publicdateA; //ok：基类的公共成员在派生类中为公共成员，对对象可见 C objC; a = objC.privatedateA; //error：基类中私有成员在派生类中是不可见的,对对象不可见 a = objC.protecteddateA; //error：基类的保护成员在派生类中为私有成员，对对象不可见 a = objC.publicdateA; //error：基类的公共成员在派生类中为私有成员，对对象不可见 D objD; a = objD.privatedateA; //error：基类中私有成员在派生类中是不可见的,对对象不可见 a = objD.protecteddateA; //error：基类的保护成员在派生类中为保护成员，对对象不可见 a = objD.publicdateA; //error：基类的公共成员在派生类中为保护成员，对对象不可见 return 0;}","link":"/posts/d1132df8/"},{"title":"DnA-LeetCode-10-正则表达式匹配-动态规划","text":"https://xlindo.com licensed under CC BY-NC-SA 4.0 1 正则表达式匹配 （LC10 困难） https://leetcode-cn.com/problems/regular-expression-matching/ 给你一个字符串 s 和一个字符规律 p，请你来实现一个支持 '.' 和 '*' 的正则表达式匹配。 '.' 匹配任意单个字符 '*' 匹配零个或多个前面的那一个元素 示例 1: 输入:s = &quot;aa&quot;p = &quot;a&quot; 输出: false 解释: &quot;a&quot; 无法匹配 &quot;aa&quot; 整个字符串。 示例 2: 输入:s = &quot;aa&quot;p = &quot;a*&quot; 输出: true 解释: 因为 '*' 代表可以匹配零个或多个前面的那一个元素, 在这里前面的元素就是 'a'。因此，字符串 &quot;aa&quot; 可被视为 'a' 重复了一次。 示例 3: 输入:s = &quot;ab&quot;p = &quot;.*&quot; 输出: true 解释: &quot;.*&quot; 表示可匹配零个或多个（'*'）任意字符（'.'）。 示例 4: 输入:s = &quot;aab&quot; p = &quot;c*a*b&quot; 输出: true 解释: 因为 '*' 表示零个或多个，这里 'c' 为 0 个, 'a' 被重复一次。因此可以匹配字符串 &quot;aab&quot;。 示例 5: 输入:s = &quot;mississippi&quot; p = &quot;mis*is*p*.&quot; 输出: false 2 解题思路 - 动态规划解法 这个题真是费了很多心思。 最重要的一步是确定状态转移方程。 2.1 预设 res[j][i]，p[0:j]与s[0:i]的匹配结果，True或者False p[j]，p的第j个字符，转移表中的行坐标 s[i]，s的第i个字符，转移表中的列坐标 要建立的转移矩阵大概是下表这个感觉 s[0] s[1] … s[n] p[0] T T T T p[1] T T T T … T T T T p[m] T T T T 注意，文中的i,j位置的程序实现并不一定精确，主要用来阐述相对位置。即i, j表示当前位置，而i-1, j-1表示前一个位置。 2.2 情况分析这里要考虑的点还比较琐碎，所以就以要取得一个匹配成功的结果为出发点，所举例证也都是对最后一个字符的匹配判断。这，要考虑两大类，总共4小类情况： p[j]不是'*'（情况1），即p[j] == 'a~z' or p[j] == '.'，此时只比较对应位置的单个字符。如果匹配，则只要之前的串能匹配，即res[j-1][i-1]为True，那么res[j][i]为True，否则为False。代码为：123if p[j] != '*': if s[i] == p[j] or p[j] == '.': res[j][i] = res[j-1][i-1] **p[j]为'*'**：因为'[a~z]*' or '.*'可以匹配零个或多个字符，所以要考虑分析p中'*'与上一个字符的组合，即'[]*'与当下s[i]的匹配情况，所以总是会考虑p[j-1] ~ s[i]。例如： 0次匹配（情况2）： 这种情况简单，例如s = &quot;aa&quot;, p = &quot;aac*&quot;，这里使用'c*'匹配了0次。又如s = &quot;a&quot;, p = &quot;aa*&quot;中的'a*'匹配0次。后面为了便于程序实现，将情况2.2纳入了情况3的分析。 前一例 （情况2.1），因为p[j-1] != s[i]，'[]*'组合实际上没有用，所以当前位置的结果就是组合之前的结果，就是p[0:j-2]与s[0:i]的匹配结果，**res[j][i] = res[j-2][i]；** 第二例 （情况2.2），虽然p[j-1] == s[i]，但'[]*'组合实际上没有作用，同样地，**res[j][i] = res[j-2][i]。** 多次匹配（情况3）： 例如s = &quot;aaa&quot;, p = &quot;a*&quot;，这里'a*'匹配了3次'a'。 此时，只要s[i] == s[i-1]，那么p中'[]*'组合要是能匹配前一个位置的s[i-1]，就也能匹配当前位置的s[i]。所以res[j][i]的结果不变，即res[j][i] = res[j][i-1]。 综上，由于要使用清晰的判断过程实现，所以分为了4种情况。即对于每一个位置的res[j][i]，其判断流程为： 1234567891011if p[j] != '*': # 情况1 if s[i] == p[j] or p[j] == '.': res[j][i] = res[j-1][i-1]else: if p[j-1] == s[i] or p[j-1] == '.': # 情况3 or 情况2.2 res[j][i] = res[j][i-1] or res[j-2][i] else: # 情况2.1 res[j][i] = res[j-2][i] 这里还需要注意避免陷入前一步匹配结果的嵌套思考。因为动态规划的思想即是当下的推理就是建立在之前的结果之上的，考虑好当下的结果和转移方程，就是考虑了之后的判断基础。 2.3 程序实现上的考虑上面虽然考虑到了各种情况，但在实际的实现中，还需要 将要建立的表扩大一个位置，以表示p[0:0]和s[0:0]的匹配结果，res[0][0]总为True； 在输入的字符串前加个相同的字符，比如空格' '。 第一点很好理解，因为不那么做，判断[j-2]的时候可能就会溢出。这里说一下第二点。 例如，s = &quot;aa&quot;, p = &quot;a*&quot;，按照第一点和之前的推导，建立的$3\\times 3$（不考虑标题栏，表中0为False，1为True）的表在res[2][1]将会是0，而这是不对的。 ‘a’ ‘a’ 1 0 0 ‘a’ 0 1 ‘*’ 0 0 因为此时，还没有res[j][i-1] or res[j-2][i]的结果，其判断依据是初始化矩阵时填的0，这是错误的，尤其是对于p=[]*的情况，正确的值应该为1。 所以，若要给出正确的初始填值，考虑给一个相同的首字符来制造初始判断根据，这将不会影响最终的结果。于是对于s = &quot; aa&quot;, p = &quot; a*&quot;，建立的$4\\times 4$的表为 ‘ ‘ ‘a’ ‘a’ 1 0 0 0 ‘ ‘ 0 1 0 0 ‘a’ 0 0 1 0 ‘*’ 0 1 1 1 可以看到，&quot; a*&quot;与&quot; &quot;的匹配结果为1，给后面的判断提供了正确的初始依据。 3 代码代码的坐标最终没有改成特别舒服的写法，其中p和j总是对应为某行，而s和i总对应为某列。因为循环从1开始，所以s, p的坐标要-1。 123456789101112131415161718192021class Solution: def isMatch(self, s: str, p: str) -&gt; bool: s = ' ' + s p = ' ' + p len_s, len_p = len(s), len(p) res = [[0 for _ in range(len_s+1)] for _ in range(len_p+1)] res[0][0] = 1 for i in range(1, len_s+1): for j in range(1, len_p+1): if p[j-1] != '*': if s[i-1] == p[j-1] or p[j-1] == '.': res[j][i] = res[j-1][i-1] else: if p[j-2] == s[i-1] or p[j-2] == '.': res[j][i] = res[j][i-1] or res[j-2][i] else: res[j][i] = res[j-2][i] return True if res[-1][-1] else False 应该所有情况都是考虑到了。若有不对的地方，请务必帮助我指出，谢谢。","link":"/posts/8d34e813/"},{"title":"How about RISC-V?","text":"2022, I am still alive. FYI, see about. This site will still archive some important articles I wrote, but not frequently. Starting in March, I became an AI acceleration engineer at a RISC-V startup, looking for opportunities at the crossroads of AI and RISC-V-based hardware. This job is challenging but interesting. In addition, RISC-V, with its openness, feasibility and advancement, is also undoubtedly the future of growing hardware world. Hope it is a worthwhile direction to push AI in RISC-V.","link":"/posts/4961c61b/"},{"title":"MATLAB-Symbolic Math Toolbox 符号计算","text":"https://xlindo.com licensed under CC BY-NC-SA 4.0 按官网摘抄一些精华，快速上手。第2节比较重要。 Getting Started with Symbolic Math Toolbox 1 创建符号计算对象该工具箱包含的运算对象为以下5个，可使用多种方法进行创建，核心函数为sym和syms. Symbolic Numbers Symbolic Variables Symbolic Expressions Symbolic Functions Symbolic Matrices 1.1 Symbolic Numbers1234567&gt;&gt; sym(1/3)&gt;&gt; ans =1/3&gt;&gt; 1/3ans = 0.3333 1.2 Symbolic Variables1234567&gt;&gt; syms x;&gt;&gt; y = sym('y'); &gt;&gt; whos Name Size Bytes Class Attributes x 1x1 8 sym y 1x1 8 sym 12345678910&gt;&gt; A = sym('a', [1 20]) A = [ a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, a14, a15, a16, a17, a18, a19, a20] &gt;&gt; whos Name Size Bytes Class Attributes A 1x20 8 sym 1234567&gt;&gt; syms(sym('a', [1 10]))&gt;&gt; whos Name Size Bytes Class Attributes a1 1x1 8 sym a10 1x1 8 sym ...etc... 12345&gt;&gt; sym('a_%d',[1,10]) ans = [ a_1, a_2, a_3, a_4, a_5, a_6, a_7, a_8, a_9, a_10] 1.3 Symbolic ExpressionsFor $\\phi = \\frac{1+\\sqrt{5}}{2}$, 1234567891011&gt;&gt; phi = (1 + sqrt(sym(5)))/2 phi = 5^(1/2)/2 + 1/2 &gt;&gt; f = phi^2 - phi - 1 f = (5^(1/2)/2 + 1/2)^2 - 5^(1/2)/2 - 3/2 12345&gt;&gt; ff = str2sym('a^2+b^2') ff = a^2 + b^2 1.4 Symbolic Functions Use symbolic functions that accept symbolic inputs, such as $f(x,y)$. 123456&gt;&gt; syms f(x,y)&gt;&gt; f f(x, y) = f(x, y) Assign a mathematical expression to f. 1234567891011&gt;&gt; f(x,y) = x^2*y f(x, y) = x^2*y &gt;&gt; f f(x, y) = x^2*y Find the value of f at (3,2). 12345&gt;&gt; f(3,2) ans = 18 Find the value of array inputs. 12345&gt;&gt; xVal = 1:5; yVal = 3:7; f(xVal,yVal) ans = [ 3, 16, 45, 96, 175] You can differentiate symbolic functions, integrate or simplify an expression.The result is also a symbolic function.Differentiate, 12345&gt;&gt; dfx = diff(f,x) dfx(x, y) = 2*x*y 1.5 Symbolic Matrices Use matrices containing symbolic values. Use Existing Symbolic Variables 12345678&gt;&gt; syms a b cA = [a b c; c a b; b c a] A = [ a, b, c][ c, a, b][ b, c, a] To check if the sum of the elements of the first row equals the sum of the elements of the second column, use the isAlways function: isAlways(cond) checks if the condition cond is valid for all possible values of the symbolic variables in cond. 1234567&gt;&gt; isAlways(sum(A(1,:)) == sum(A(:,2)))ans = logical 1 Generate Elements While Creating a Matrix 12345678910111213&gt;&gt; A = sym('A', [2 4]) A = [ A1_1, A1_2, A1_3, A1_4][ A2_1, A2_2, A2_3, A2_4] &gt;&gt; A = sym('A%d%d', [2 4]) A = [ A11, A12, A13, A14][ A21, A22, A23, A24] Create Matrix of Symbolic Numbers 123456789101112131415&gt;&gt; A = hilb(3)A = 1.0000 0.5000 0.3333 0.5000 0.3333 0.2500 0.3333 0.2500 0.2000&gt;&gt; A = sym(A) A = [ 1, 1/2, 1/3][ 1/2, 1/3, 1/4][ 1/3, 1/4, 1/5] 2 进行符号计算 Perform computations on symbolic objects. 2.1 求导，求微分 Differentiate Symbolic Expressions 通过该工具箱，可以进行多种符号对象的多种求导计算： Derivatives of single-variable expressions Partial derivatives Second and higher order derivatives Mixed derivatives 2.1.1 单变量1234567&gt;&gt; syms xf = sin(x)^2; diff(f) ans = 2*cos(x)*sin(x) 2.1.2 偏导Specify the differentiation variable, 1234567&gt;&gt; syms x yf = sin(x)^2 + cos(y)^2;diff(f, y) ans = -2*cos(y)*sin(y) 2.1.3 高阶导12345&gt;&gt; syms x; f = x^4; diff(f, x, 3) ans = 24*x 1234567&gt;&gt; syms x yf = sin(x)^2 + cos(y)^2;diff(f, y, 2) ans = 2*sin(y)^2 - 2*cos(y)^2 2.1.4 混合求导123456789101112131415&gt;&gt; syms x yf = sin(x)^2 + cos(y)^2;diff(f, y) ans = -2*cos(y)*sin(y) &gt;&gt; syms x yf = sin(x)^2 + cos(y)^2;diff(diff(f, y), x) ans = 0 2.2 求积分 Indefinite and definite integration Integration of multivariable expressions 2.2.1 单变量不定积分 Indefinite Integrals of One-Variable Expressions 1234567&gt;&gt; syms xf = sin(x)^2;&gt;&gt; int(f) ans = x/2 - sin(2*x)/4 2.2.2 多变量不定积分 Indefinite Integrals of Multivariable Expressions 1234567&gt;&gt; syms x y nf = x^n + y^n;int(f,x) ans = x*y^n + (x*x^n)/(n + 1) 1234567&gt;&gt; syms x y nf = x^n + y^n;int(f, n) ans = x^n/log(x) + y^n/log(y) 2.2.3 定积分 Definite Integrals 1234567891011&gt;&gt; syms x; f = x^2; int(f, x) ans = x^3/3 &gt;&gt; int(f,x,1,2) ans = 7/3 1234567&gt;&gt; syms x y nf = x^n + y^n;int(f, 1, 10) ans = piecewise(n == -1, log(10) + 9/y, n ~= -1, (10*10^n - 1)/(n + 1) + 9*y^n) 2.2.4 没有封闭形式解 Cannot Find a Closed Form of an Integral It returns an unresolved integral, 123456&gt;&gt; syms xint(sin(sinh(x))) ans = int(sin(sinh(x)), x) 2.3 解方程 Solve Equations延申Equation Solving 该工具箱可以求解多种类型的方程： Algebraic equations with one symbolic variable Algebraic equations with several symbolic variables Systems of algebraic equations 2.3.1 单变量12345678&gt;&gt; syms xsolve(x^3 - 6*x^2 == 6 - 11*x) ans = 1 2 3 It assumes the undefined right side is zero: 12345678&gt;&gt; syms xsolve(x^3 - 6*x^2 + 11*x - 6) ans = 1 2 3 2.3.2 多变量可以指定求解某变量关于另一变量的解表达式， 12345678&gt;&gt; syms x ysolve(6*x^2 - 6*x^2*y + x*y^2 - x*y + y^3 - y^2 == 0, y) ans = 1 2*x -3*x 2.3.3 方程组12345678910111213141516171819&gt;&gt; syms x y z[x, y, z] = solve(z == 4*x, x == y, z == x^2 + y^2) x = 0 2 y = 0 2 z = 0 8 2.4 化简 Simplify Symbolic Expressions 2.4.1 simplify(f)化简12345678910111213&gt;&gt; syms x&gt;&gt; phi = (1 + sqrt(x))/2;&gt;&gt; f = phi^2 - phi - 1 f = (x^(1/2)/2 + 1/2)^2 - x^(1/2)/2 - 3/2 &gt;&gt; simplify(f) ans = x/4 - 5/4 2.4.2 factor(f) 因式分解1234567&gt;&gt; syms xg = x^3 + 6*x^2 + 11*x + 6;factor(g) ans = [ x + 3, x + 2, x + 1] 2.4.3 expand(f) 展开为多项式标准形式1234567&gt;&gt; syms xf = (x ^2- 1)*(x^4 + x^3 + x^2 + x + 1)*(x^4 - x^3 + x^2 - x + 1);expand(f) ans = x^10 - 1 2.4.4 horner(f) 转为嵌套形式1234567&gt;&gt; syms xh = x^5 + x^4 + x^3 + x^2 + x;horner(h) ans = x*(x*(x*(x*(x + 1) + 1) + 1) + 1) 2.5 将值代入表达式 Substitutions in Symbolic Expressions 2.5.1 代入数值1234567&gt;&gt; syms xf = 2*x^2 - 3*x + 1;subs(f, 1/3) ans = 2/9 指定变量代入， 1234567&gt;&gt; syms x yf = x^2*y + 5*x*sqrt(y);&gt;&gt; subs(f, x, 3) ans = 9*y + 15*y^(1/2) 2.5.2 代入符号变量1234567&gt;&gt; syms x yf = x^2*y + 5*x*sqrt(y);&gt;&gt; subs(f, y, x) ans = x^3 + 5*x^(3/2) 12345678910111213141516171819&gt;&gt; syms a b cA = [a b c; c a b; b c a] A = [ a, b, c][ c, a, b][ b, c, a] &gt;&gt; alpha = sym('alpha');beta = sym('beta');A(2,1) = beta;A = subs(A,b,alpha) A = [ a, alpha, c][ beta, a, alpha][ alpha, c, a] 2.5.3 代入矩阵 Substitute a Matrix into a Polynomial 2.5.3.1 Element-by-Element 代入即按矩阵单个元素依次代入， 123456789&gt;&gt; syms xf = x^3 - 15*x^2 - 24*x + 350;A = [1 2 3; 4 5 6];subs(f,A) ans = [ 312, 250, 170][ 78, -20, -118] 2.5.3.2 矩阵整体代入使用sym2poly(f)将原符号表达式转换为多项式b，然后直接使用polyvalm(b,A)求解。也可以重写符号表达式对应的矩阵形式，再代入矩阵求解。 12345678910111213141516171819202122232425262728293031&gt;&gt; syms xf = x^3 - 15*x^2 - 24*x + 350;&gt;&gt; A = magic(3)A = 8 1 6 3 5 7 4 9 2&gt;&gt; b = sym2poly(f)b = 1 -15 -24 350&gt;&gt; polyvalm(b,A)ans = -10 0 0 0 -10 0 0 0 -10 &gt;&gt; A^3 - 15*A^2 - 24*A + 350*eye(3)ans = -10 0 0 0 -10 0 0 0 -10 2.6 作图 Plot Symbolic Functions 该工具箱提供的作图函数包括， fplot to create 2-D plots of symbolic expressions, equations, or functions in Cartesian coordinates. fplot3 to create 3-D parametric plots. ezpolar to create plots in polar coordinates. fsurf to create surface plots. fcontour to create contour plots. fmesh to create mesh plots. 2.6.1 显式函数 Explicit Function Plot Plot $x^3−6x^2+11x−6$using fplot, 12345678&gt;&gt; syms xf = x^3 - 6*x^2 + 11*x - 6;fplot(f)&gt;&gt; xlabel('x')ylabel('y')title(texlabel(f))grid on 2.6.2 隐式函数 Implicit Function Plot Plot $(x^2+y^2)^4=(x^2−y^2)^2$ using fimplicit, 123&gt;&gt; syms x yeqn = (x^2 + y^2)^4 == (x^2 - y^2)^2;fimplicit(eqn, [-1 1]) 2.6.3 作三维图 3-D Plot Plot$$\\begin{cases} x=t^2\\sin(10t)\\ y=t^2\\cos(10t)\\ z=t\\end{cases}$$using fplot3, 12&gt;&gt; syms tfplot3(t^2*sin(10*t), t^2*cos(10*t), t) 2.6.4 作曲面图 Create Surface Plot Plot $x^2 + y^2$ using fsurf, 12&gt;&gt; syms x yfsurf(x^2 + y^2) 3 使用假设限定 Use Assumptions on Symbolic Variables 3.1 默认假设工具箱默认符号变量为不带假设，使用assumptions(z)检查，若返回空，即不带任何假设， 12345678910111213&gt;&gt; syms z&gt;&gt; assumptions(z) ans = Empty sym: 1-by-0&gt;&gt; assume(z&gt;=0)&gt;&gt; assumptions(z) ans = 0 &lt;= z 3.2 设置假设上例中已使用assume()设置了假设，也可以使用其他方式设置假设， 123456&gt;&gt; assumeAlso(x,'integer')&gt;&gt; assumptions(x) ans = in(x, 'integer') 12345678&gt;&gt; a = sym('a', 'real');b = sym('b', 'real');c = sym('c', 'positive');&gt;&gt; assumptions(a) ans = in(a, 'real') 12345678&gt;&gt; syms a b realsyms c positive&gt;&gt; assumptions(c) ans = 0 &lt; c 3.3 删除假设注意，在使用clear x语句删除假设时：使用syms创建的带有假设的符号被删除后，若使用sym再次创建该符号，该符号会保持之前的假设。若要彻底删除假设，需要使用syms创建并再次删除。 即理论上，下式应有复数解，但sym使其保持了之前的实数假设， 12345678&gt;&gt; syms x real&gt;&gt; clear x&gt;&gt; x = sym('x');&gt;&gt; solve(x^2 + 1 == 0, x) ans = Empty sym: 0-by-1 若要去除假设，需使用syms创建变量， 123456789&gt;&gt; syms x realclear xsyms x;solve(x^2 + 1 == 0, x) ans = -1i 1i","link":"/posts/2d4d0a06/"},{"title":"Keypoints in short","text":"https://xlindo.com所有文章遵循CC BY-NC-SA 4.0许可协议。请按许可使用。Blog content follows the CC BY-NC-SA 4.0 License. Keypoints to look up. Updating… Effects of Poles and Zeros/wiki/Control_Systems/Poles_and_Zeros As s approaches a zero, the numerator of the transfer function (and therefore the transfer function itself) approaches the value 0. When s approaches a pole, the denominator of the transfer function approaches zero, and the value of the transfer function approaches infinity. An output value of infinity should raise an alarm bell for people who are familiar with BIBO stability. We will discuss this later. As we have seen above, the locations of the poles, and the values of the real and imaginary parts of the pole determine the response of the system. Real parts correspond to exponentials, and imaginary parts correspond to sinusoidal values. Addition of poles to the transfer function has the effect of pulling the root locus to the right, making the system less stable. Addition of zeros to the transfer function has the effect of pulling the root locus to the left, making the system more stable. QR decomposition/wiki/QR_decomposition In linear algebra, a QR decomposition (also called a QR factorization) of a matrix is a decomposition of a matrix A into a product A = QR of an orthogonal matrix Q and an upper triangular matrix R. QR decomposition is often used to solve the linear least squares problem and is the basis for a particular eigenvalue algorithm, the QR algorithm. Any real square matrix A may be decomposed as $$A=QR$$ where $Q$ is an orthogonal matrix (its columns are orthogonal unit vectors meaning $Q^TQ=QQ^T=I$ and $R$ is an upper triangular matrix (also called right triangular matrix). If $A$ is invertible, then the factorization is unique if we require the diagonal elements of $R$ to be positive. Autocovariance/wiki/Autocovariance In probability theory and statistics, given a stochastic process, the autocovariance is a function that gives the covariance of the process with itself at pairs of time points. Autocovariance is closely related to the autocorrelation of the process in question. $$\\gamma(i, j)=E\\left[\\left(X_{i}-\\mu_{i}\\right)\\left(X_{j}-\\mu_{j}\\right)\\right]$$ For a time series $$\\gamma(k)=E\\left[\\left(X_{i}-\\mu\\right)\\left(X_{i-k}-\\mu\\right)\\right]$$ Trace of a matrix and eigenvaluesFirst,$$tr(\\boldsymbol{A}) = sum(eig(\\boldsymbol{A}))$$ Derivation: for one-dimensional n-order equation, the sum of solutions equals to the negative coefficient of the second highest order item. e.g. $$\\left(x+x_{1}\\right)\\left(x+x_{2}\\right) \\cdots\\left(x+x_{n}\\right)=0$$ $$x^{n}+\\left(x_{1}+x_{2}+\\cdots+x_{n}\\right) x^{n-1}+\\cdots+x_{1} x_{2} \\cdots x_{n}=0$$ We have $\\det(\\lambda \\boldsymbol{I} - \\boldsymbol{A}) = 0,$ then we get$$\\prod_i(\\lambda - a_{ii}) + \\cdots = 0,$$which gives$$\\lambda^{n}-\\left(\\lambda_{1}+\\lambda_{2}+\\cdots+\\lambda_{n}\\right) \\lambda^{n-1}+\\cdots+\\lambda_{1} \\lambda_{2} \\cdots \\lambda_{n}=0.$$ The negative coefficient of the second highest item equals $\\lambda_{1}+\\lambda_{2}+\\cdots+\\lambda_{n}$, which is the trace of matrix $\\boldsymbol{A}$. Lagrange function$$\\begin{align}\\max_{x} &amp; \\quad f(x) \\s.t. &amp; \\quad g_i(x) \\leqslant 0, \\quad i=1,2,\\cdots,k \\&amp;\\quad h_j(x)=0,\\quad j=1,2,\\cdots,l\\end{align}$$ The Lagrange function,$$\\mathcal{L}(\\boldsymbol{x}, \\boldsymbol{\\alpha}, \\boldsymbol{\\beta}) =f(\\boldsymbol{x})+\\sum_{i=1}^{m} \\alpha_{i} g_{i}(\\boldsymbol{x})+\\sum_{j=1}^{n} \\beta_{j} h_{j}(\\boldsymbol{x}), ;;\\alpha \\geqslant 0$$then,$$\\begin{aligned} \\min {\\boldsymbol{x}} \\max {\\boldsymbol{\\alpha}, \\boldsymbol{\\beta}} \\mathcal{L}(\\boldsymbol{x}, \\boldsymbol{\\alpha}, \\boldsymbol{\\beta}) = &amp; \\min {\\boldsymbol{x}}\\left(f(\\boldsymbol{x})+\\max {\\boldsymbol{\\alpha}, \\boldsymbol{\\beta}}\\left(\\sum{i=1}^{m} \\alpha{i} g{i}(\\boldsymbol{x})+\\sum{j=1}^{n} \\beta_{j} h_{j}(\\boldsymbol{x})\\right)\\right) \\[1ex] = &amp; \\min_{\\boldsymbol{x}}\\left(f(\\boldsymbol{x})+\\left{\\begin{array}{l}{0},, &amp; \\boldsymbol{x} , \\text{meet constraints} \\ {\\infty},, &amp; \\text{otherwise}\\end{array}\\right.\\right) \\[1ex] = &amp; \\min_{\\boldsymbol{x}} f(\\boldsymbol{x}), ;;\\boldsymbol{x} , \\text{meet constraints}\\end{aligned}$$ Softmax vs Sigmoid Softmax Sigmoid formular $\\sigma (\\mathbf {z} ){j}={\\frac {e^{z{j}}}{\\sum {k=1}^{K}e^{z{k}}}}$ $S(x)={\\frac {1}{1+e^{-x}}}$ Essential 离散概率分布 非线性映射 Task 多分类 二分类 定义域 某个一维向量 单个数值 值域 [0,1] (0,1) 结果之和 一定为 1 为某个正数 Sigmoid is a special case (2 categories) of Softmax.","link":"/posts/e2693f90/"},{"title":"MATLAB-加速MATLAB代码","text":"https://xlindo.com所有文章遵循CC BY-NC-SA 4.0许可协议。请按许可使用。Blog content follows the CC BY-NC-SA 4.0 License. 原题：How to speed up MATLAB code? 来源：MATLAB官网视频 2019年7月29日 1 例一 循环条件赋值有代码，tips3.m， 1234567891011121314clear('A','B');numElements = 5000;for i = 1:numElements for j = 1:numElements A(i,j) = rand; if (A(i,j) &gt; 0.5) B(i,j) = A(i,j) else B(i,j) = -A(i,j) end endend 查看运行时间，使用tic，toc， 12&gt;&gt; tic; tips3; toc时间已过 130.978966 秒。 查看右侧滑块条，红色表示错误，橙色表示可改进，绿色表示就绪， 上图可以看出，在有A，B的地方有橙色指示，表示每次他们的大小的改变，都是昂贵的。，特别是大矩阵。 1.1 预分配解决方法是预分配。如加上A = zeros(numElements); 再测时间， 12&gt;&gt; tic; tips3; toc时间已过 1.641815 秒。 1.2 探查器与循环外赋值现在指示已转为绿色，我们使用**MATLAB探查器(Profiler)**衡量代码核心性能，来进行进一步加速。 转到主页(Home)，选择运行并计时(Run and Time)，这将打开探查器，使用其进行分析， 我的2019a显示空白 解决方法为：Lxknn 打开主页→预设→字体→自定义，在右侧“桌面工具”中找到“探查器”，修改字体为Segoe UI，之后点击下面的“应用”就好。不行的话试试其他字体。适用于MATLAB R2019a中文版。 可以查看很多信息。同时，我们在下面逐行分析中发现耗时最多的语句为A(i,j)， 于是，将rand改为循环外的rand(n)，这时，运行探查器的总时间缩短为7s。 1.3 逻辑数组条件赋值接下来使用逻辑数组改善循环的判断与条件赋值， 1234567clear('A','B');numElements = 5000;A = rand(numElements);B = A;idx = A &lt; 0.5;B(idx) = -A(idx); 现在的总时长陡降至1s以内，而且可以看到，没有循环。 至此我们的优化使程序从100+s -&gt; 1-s。 2 例二 矩阵最大特征值复用例一的代码，同时进行矩阵B的最大特征值的计算。 12345678910111213clear('A','B','C');numElements = 5000;numB = 24;C = zeros(1, numB);for i = 1:numB A = rand(numElements); B = A; idx = A &lt; 0.5; B(idx) = -A(idx); C(:,i) = max(eig(B));end 同样的，查看一下运行时间， 12&gt;&gt; tic;tips3;toc时间已过 1258.652858 秒。 可以看到循环内每次计算特征值都非常耗时，这种情况下可以进行多核的并行计算后合并结果。 使用Parallel Computing Toolbox完成该项工作。 使用parpool创建worker池，worker数量取决于CPU核心数， 1&gt;&gt; parpool 然后使用parfor代替for，即parfor i = 1:numB，可以看到最终时间， 1234&gt;&gt; tic;tips3;toc时间已过 813.310429 秒。IdleTimeout has been reached.Parallel pool using the 'local' profile is shutting down. 注意，并行计算将带来其他的开销，例如创建worker的时间，并行代码的编写，通信开销等。建议使用时先测试并确定并行计算是值得的。 3 其他方法https://ww2.mathworks.cn/company/newsletters/articles/accelerating-matlab-algorithms-and-applications.html","link":"/posts/a7a8dc40/"},{"title":"ML - kNN,k-近邻算法学习","text":"https://xlindo.com所有文章遵循CC BY-NC-SA 4.0许可协议。请按许可使用。 内容：k-近邻算法Python实现 （《机器学习实战》2013年6月第1版第2章）代码注释：xlindo 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108####################################################################### 注释编写人：xlindo （http://xlindo.com）# 内容：k-近邻算法Python实现 （《机器学习实战》2013年6月第1版第2章）# 作者：Peter Harrington# 主要函数功能：# classify0(inX,dataSet,labels,k) --&gt; kNN算法实现# file2matrix(filename) --&gt; 将文本内容转换为dataSet,Labels# autoNorm(dataSet) --&gt; 归一化至[0,1]# datingClassTest() --&gt; 测试datingClass算法# img2vector(filename) --&gt; 将32*32的二进制图转换为1*1024数组# handwritingClassTest() --&gt;测试手写识别算法######################################################################from numpy import * #必要的数学库import operatorfrom os import listdir #获取文件夹下文件名用def classify0(inX,dataSet,labels,k): #kNN算法实现，inX:输入向量，dataSet:训练样本集，labels:标签向量，k:选择近邻数 dataSetSize = dataSet.shape[0] #numpy方法，获取选定维度的总数 diffMat = tile(inX, (dataSetSize,1)) - dataSet #tile方法：重复数组到行、列，创建数组 sqDiffMat = diffMat ** 2 sqDistances = sqDiffMat.sum(axis=1) #计算选定维度的和 distances = sqDistances ** 0.5 sortedDistIndicies = distances.argsort() #argsort方法：返回排序后数据下标的表 classCount = {} for i in range(k): voteIlable = labels[sortedDistIndicies[i]] classCount[voteIlable] = classCount.get(voteIlable,0) + 1 sortedClassCount = sorted(iter(classCount.items()), key=operator.itemgetter(1),reverse=True) #itermgetter方法：返回选定域的值 return sortedClassCount[0][0]def file2matrix(filename): #将文本内容转换为dataSet,Labels fr = open(filename) arrayOLines = fr.readlines() numberOfLines = len(arrayOLines) returnMat = zeros((numberOfLines,3)) #numpy.zeros方法：创建(m,n)m*n的数组 classLabelVector = [] index = 0 for line in arrayOLines: line = line.strip() listFromLine = line.split('\\t') returnMat[index,:] = listFromLine[0:3] classLabelVector.append(int(listFromLine[-1])) index += 1 return returnMat,classLabelVectordef autoNorm(dataSet): #归一化至[0,1]，公式为newValue=(oldValue-min)/(max-min) minVals = dataSet.min(0) #参数0表示从列中选 maxVals = dataSet.max(0) ranges = maxVals - minVals normDataSet = zeros(shape(dataSet)) m = dataSet.shape[0] normDataSet = dataSet - tile(minVals,(m,1)) normDataSet = normDataSet/tile(ranges,(m,1)) return normDataSet,ranges,minValsdef datingClassTest(): #测试datingClass算法正确率 hoRatio = 0.10 datingDataMat,datingLabels = file2matrix('datingTestSet2.txt') normMat,ranges,minVals = autoNorm(datingDataMat) m = normMat.shape[0] numTestVecs = int(m*hoRatio) errorCount = 0.0 for i in range(numTestVecs): classifierResult = classify0(normMat[i,:],normMat[numTestVecs:m,:], datingLabels[numTestVecs:m],3) print(&quot;the classifier's answer is: %d, the right answer is: %d&quot; %(classifierResult,datingLabels[i])) if classifierResult!=datingLabels[i]: errorCount += 1.0 print(&quot;error ratio = %f&quot;%(errorCount/numTestVecs))def img2vector(filename): #读取文件，将32*32的二进制图转换为1*1024数组 returnVect = zeros((1,1024)) fr = open(filename) for i in range(32): lineStr = fr.readline() for j in range(32): returnVect[0,32*i+j] = int(lineStr[j]) fr.close() return returnVectdef handwritingClassTest(): #测试手写识别算法 hwLabels = [] trainingFileList = listdir('trainingDigits') m = len(trainingFileList) trainingMat = zeros((m,1024)) for i in range(m): fileNameStr = trainingFileList[i] fileStr = fileNameStr.split('.')[0] classNumStr = int(fileStr.split('_')[0]) hwLabels.append(classNumStr) trainingMat[i,:] = img2vector('trainingDigits/%s' % fileNameStr ) testFileList = listdir('testDigits') errorCount = 0.0 mTest = len(testFileList) for i in range(mTest): fileNameStr = testFileList[i] fileStr = fileNameStr.split('.')[0] classNumStr = int(fileStr.split('_')[0]) vectorUnderTest = img2vector('testDigits/%s' % fileNameStr) classifierResult = classify0(vectorUnderTest,trainingMat,hwLabels,3) print(&quot;result= %d , answer= %d &quot; % (classifierResult,classNumStr)) if classifierResult != classNumStr: errorCount += 1.0 print(&quot;error ratio = %f&quot;,errorCount/mTest)","link":"/posts/18e427db/"},{"title":"Math - Fisher Information","text":"https://xlindo.com所有文章遵循CC BY-NC-SA 4.0许可协议。请按许可使用。Blog content follows the CC BY-NC-SA 4.0 License. Fisher information matrix is very important in my master’s thesis paper, but it is complicated than I expected before. So this blog is about to illustrate what it is and how to calculate. If time is adequate for me, I will give a Matlab example todo. chapter Fisher InformationAgustinus Kristiadi’s BlogA Tutorial on Fisher Informationwiki/Fisher_information 0 IntroductionFisher information, From Wikipedia says the Fisher information is a way of measuring the amount of information that an observable random variable X carries about an unknown parameter $\\theta$ of a distribution that models $X$. Formally, it is the variance of the score, or the expected value of the observed information. In Bayesian statistics, the asymptotic distribution of the posterior mode depends on the Fisher information and not on the prior. The role of the Fisher information in the asymptotic theory of maximum-likelihood estimation was emphasized by the statistician Ronald Fisher. The Fisher information is also used in the calculation of the Jeffreys prior, which is used in Bayesian statistics. 1 Fisher information1.1 Basic ideas Suppose you already know the Maximum Likelihood Estimation. Let $f(X; \\theta)$ be the probability density function (or probability mass function) for $X$ conditional on the value of $\\theta$. This is also the likelihood function for $\\theta$. In practice, the true value of $\\theta$ is not known and has to be inferred from the observed data. If $f$ is sharply peaked wrt. changes in $\\theta$, it is easy to indicate the “correct” value of $\\theta$ from the data, or equivalently, that the data X provides a lot of information about the parameter $\\theta$, which means not much samples need to be used. 1.2 Score In statistics, the score (or informant) is the gradient of the log-likelihood function wrt. the parameter vector. Evaluated at a particular point, the score indicates the steepness of the log-likelihood function and thereby the sensitivity to infinitesimal changes to the parameter values. If the log-likelihood function is continuous over the parameter space, the score will vanish at a local maximum or minimum; this fact is used in maximum likelihood estimation to find the parameter values that maximize the likelihood function. Formally, it is the partial derivative wrt. $\\theta$ of the natural logarithm of the likelihood function,$$s(\\theta)=\\frac{\\partial}{\\partial \\theta} \\log f(X ; \\theta),$$ Under certain regularity conditions, it can be shown that the expected value (the first moment) of the score is 0: $\\mathbb{E}(X)=\\int_{-\\infty}^{\\infty} x f(x) d x$ Consistent convergence, then: $\\frac{d}{d x} \\int f(x, y) d y=\\int \\frac{\\partial f(x, y)}{\\partial x} d y$, more roughly, theorem in physical problems. $$\\begin{aligned} \\underset{f(X ; \\theta)}{\\mathbb{E}}[s(\\theta)| \\theta] &amp;=\\underset{f(X ; \\theta)}{\\mathbb{E}}\\left[\\frac{\\partial}{\\partial \\theta} \\log f(X ; \\theta)| \\theta\\right] \\ &amp;=\\int \\frac{\\partial}{\\partial \\theta} \\log f(x ; \\theta) f(x ; \\theta) \\mathrm{d} x \\ &amp;=\\int \\frac{\\frac{\\partial}{\\partial \\theta} f(x ; \\theta)}{f(x ; \\theta)} f(x ; \\theta) d x \\ &amp;=\\frac{\\partial}{\\partial \\theta} \\int f(x ; \\theta) d x \\ &amp;=\\frac{\\partial}{\\partial \\theta} 1 \\ &amp;=0 \\end{aligned}$$ 1.3 Covariance of score - Fisher informationWe can define an uncertainty measure around the expected estimate. That is, we look at the covariance of score of our model, which is known as Fisher information.$$\\begin{aligned}\\mathcal{I}(\\theta) &amp;=\\underset{f(X ; \\theta)}{\\mathbb{E}}\\left[(s(\\theta)-0)(s(\\theta)-0)^{\\mathrm{T}}| \\theta\\right]\\ &amp;=\\underset{f(X ; \\theta)}{\\mathbb{E}}\\left[\\left(\\frac{\\partial}{\\partial \\theta} \\log f(X ; \\theta)\\right)^{2} | \\theta\\right]\\ &amp;=\\int\\left(\\frac{\\partial}{\\partial \\theta} \\log f(x ; \\theta)\\right)^{2} f(x ; \\theta) d x\\end{aligned}$$ If $\\log{f(x; \\theta)}$ is twice differentiable with respect to $\\theta$, and under certain regularity conditions, then the Fisher information may also be written as$$\\mathcal{I}(\\theta)=-\\mathrm{E}\\left[\\frac{\\partial^{2}}{\\partial \\theta^{2}} \\log f(X ; \\theta) | \\theta\\right],$$ Besides, the likelihood function is complicated, so Empirical Fisher is defined as,$$\\mathcal{I}(\\theta)=\\frac{1}{N} \\sum_{i=1}^{N} \\nabla \\log f\\left(x_{i} | \\theta\\right) \\nabla \\log f\\left(x_{i} | \\theta\\right)^{\\mathrm{T}}.$$ 1.4 Fisher and Hessian$$\\mathcal{I}(\\theta)=-\\underset{f(X ; \\theta)}{\\mathbb{E}}\\left[\\mathbf{H}_{\\log f(X ; \\theta)}\\right]$$ where$$\\begin{aligned}\\mathrm{H}{\\log f(X | \\theta)}&amp;=\\mathrm{J}\\left(\\frac{\\nabla f(X ; \\theta)}{f(X ; \\theta)}\\right) \\ &amp;= \\frac{\\mathrm{H}{f(X ; \\theta)}}{f(X ; \\theta)}-\\left(\\frac{\\nabla f(X ; \\theta)}{f(X ; \\theta)}\\right)\\left(\\frac{\\nabla f(X ; \\theta)}{f(X ; \\theta)}\\right)^{\\mathrm{T}}\\end{aligned}$$ 1.5 ResultsThus, the Fisher information may be seen as the curvature of the support curve (the graph of the log-likelihood). Near the maximum likelihood estimate, low Fisher information therefore indicates that the maximum appears “blunt”, that is, the maximum is shallow and there are many nearby values with a similar log-likelihood. Conversely, high Fisher information indicates that the maximum is sharp. One of the most exciting results of $\\mathcal{I}(\\theta)$ is that it has connection to KL-divergence (Kullback–Leibler divergence, also called relative entropy). This gives rise to natural gradient method. 2 Fisher information matrix (FIM) https://en.wikipedia.org/wiki/Fisher_information 2.1 DefinitionWhen there are $N$ parameters, so that $\\theta$ is an $N \\times 1$ vector $\\theta ={\\begin{bmatrix}\\theta _{1},\\theta _{2},\\dots ,\\theta _{N}\\end{bmatrix}}^{\\mathrm {T} },$ then the Fisher information takes the form of an $N \\times N$ Fisher information matrix, $$[\\mathcal{I}(\\theta)]{i, j}=\\underset{f(X ; \\theta)}{\\mathbb{E}}\\left[\\left(\\frac{\\partial}{\\partial \\theta{i}} \\log f(X ; \\theta)\\right)\\left(\\frac{\\partial}{\\partial \\theta_{j}} \\log f(X ; \\theta)\\right) | \\theta\\right].$$ The FIM is a $N \\times N$ positive semidefinite matrix. If it is positive definite, then it defines a Riemannian metric on the $N$-dimensional parameter space (Information geometry). Under certain regularity conditions, the Fisher information matrix may also be written as$$[\\mathcal{I}(\\theta)]{i, j}=-\\underset{f(X ; \\theta)}{\\mathbb{E}}\\left[\\frac{\\partial^{2}}{\\partial \\theta{i} \\partial \\theta_{j}} \\log f(X ; \\theta) | \\theta\\right].$$ 2.2 Some results It can be derived as the Hessian of the relative entropy. It can be understood as a metric induced from the Euclidean metric, after appropriate change of variable. In its complex-valued form, it is the Fubini–Study metric.It is the key part of the proof of Wilks’ theorem, which allows confidence region estimates for maximum likelihood estimation (for those conditions for which it applies) without needing the Likelihood Principle. In cases where the analytical calculations of the FIM above are difficult, it is possible to form an average of easy Monte Carlo estimates of the Hessian of the negative log-likelihood function as an estimate of the FIM.[6][7][8] The estimates may be based on values of the negative log-likelihood function or the gradient of the negative log-likelihood function; no analytical calculation of the Hessian of the negative log-likelihood function is needed. [6] “Monte Carlo Computation of the Fisher Information Matrix in Nonstandard Settings” http://dx.doi.org/10.1198/106186005X78800[7] “Improved Methods for Monte Carlo Estimation of the Fisher Information Matrix” http://dx.doi.org/10.1109/ACC.2008.4586850[8] “Efficient Monte Carlo Computation of Fisher Information Matrix Using Prior Information” http://dx.doi.org/10.1016/j.csda.2009.09.018 2.3 Orthogonal parametersWhen dealing with research problems, it is very common for the researcher to invest some time searching for an orthogonal parametrization of the densities involved in the problem. 2.4 Singular statistical modelIf the Fisher information matrix is positive definite for all $\\theta$, then the corresponding statistical model is said to be regular; otherwise, the statistical model is said to be singular. Examples of singular statistical models include the following: normal mixtures, binomial mixtures, multinomial mixtures, Bayesian networks, neural networks, radial basis functions, hidden Markov models, stochastic context-free grammars, reduced rank regressions, Boltzmann machines. In machine learning, if a statistical model is devised so that it extracts hidden structure from a random phenomenon, then it naturally becomes singular. 2.5 Multivariate normal distributionThe FIM for a N-variate multivariate normal distribution, $X\\sim N\\left(\\mu (\\theta ),\\Sigma (\\theta )\\right)$ has a special form. 3 Applications3.1 Optimal design of experiments /wiki/Optimal_design Because of the reciprocity of estimator-variance and Fisher information, minimizing the variance corresponds to maximizing the information. The inverse of the variance of the estimated parameter is called the information matrix. It is complicated to minimizing the information matrix. Traditionally, statisticians have evaluated estimators and designs by considering some summary statistic of the covariance matrix (of an unbiased estimator), usually with positive real values (like the determinant or matrix trace). The traditional optimality criteria are the information matrix’s invariants, in the sense of invariant theory; algebraically, the traditional optimality criteria are functionals of the eigenvalues of the (Fisher) information matrix (see optimal design). 3.2 Jeffreys prior in Bayesian statisticsIn Bayesian statistics, the Fisher information is used to calculate the Jeffreys prior, which is a standard, non-informative prior for continuous distribution parameters. 3.3 Computational neuroscienceThe Fisher information has been used to find bounds on the accuracy of neural codes. In that case, $X$ is typically the joint responses of many neurons representing a low dimensional variable $\\theta$ (such as a stimulus parameter). In particular the role of correlations in the noise of the neural responses has been studied. 3.4 Machine learningThe Fisher information is used in machine learning techniques such as elastic weight consolidation, which reduces catastrophic forgetting in artificial neural networks. 3.5 Relation to relative entropyIf $\\theta$ is fixed, then the relative entropy between two distributions of the same family is minimized at $\\theta ‘=\\theta$. For $\\theta ‘$ close to $\\theta$, one may expand the previous expression in a series up to second order: $$\\begin{aligned}D\\left(\\theta | \\theta^{\\prime}\\right)&amp;=\\int f(x ; \\theta) \\log \\frac{f(x ; \\theta)}{f\\left(x ; \\theta^{\\prime}\\right)} d x\\&amp;=\\int f(x ; \\theta)\\left(\\log f(x ; \\theta)-\\log f\\left(x ; \\theta^{\\prime}\\right)\\right) d x \\&amp;=\\frac{1}{2}\\left(\\theta^{\\prime}-\\theta\\right)^{\\top} \\underbrace{\\left(\\frac{\\partial^{2}}{\\partial \\theta_{i}^{\\prime} \\partial \\theta_{j}^{\\prime}} D\\left(\\theta | \\theta^{\\prime}\\right)\\right){\\theta^{\\prime}=\\theta}}{\\text { Fisher info. }}\\left(\\theta^{\\prime}-\\theta\\right)+\\cdots\\end{aligned}$$ Thus the Fisher information represents the curvature of the relative entropy. Schervish (1995: §2.3) says the following.One advantage Kullback-Leibler information has over Fisher information is that it is not affected by changes in parameterization. Another advantage is that Kullback-Leibler information can be used even if the distributions under consideration are not all members of a parametric family.…Another advantage to Kullback-Leibler information is that no smoothness conditions on the densities … are needed.","link":"/posts/c0aebc2/"},{"title":"Math- Gradient, Jacobian matrix and Hessian matrix","text":"https://xlindo.com licensed under CC BY-NC-SA 4.0 1 GradientThe gradient of $f$ is defined as the unique vector field whose dot product with any unit vector $\\mathbf{v}$at each point $x$ is the directional derivative of $f$ along $\\mathbf{v}$. That is,$$\\big (\\nabla f(x){\\big )}\\cdot \\mathbf {v} =D_{\\mathbf {v} }f(x)$$e.g. in coordinate system, 沿着$i$方向的导数，就是$i$轴方向的分量 $$\\nabla f = \\frac { \\partial f } { \\partial x _ { 1 } } e _ { 1 } + \\cdots + \\frac { \\partial f } { \\partial x _ { n } } e _ { n }$$ The relationship to derivation. The best linear approximation to a differentiable function$$f\\colon \\mathbf {R} ^{n}\\to \\mathbf {R}$$at a point $x$ in $\\mathbb{R}^n$ is a linear map from $\\mathbb{R}^n$ to $\\mathbb{R}$ which is often denoted by dfx or $Df(x)$ and called the differential or (total) derivative of f at $x$. The gradient is therefore related to the differential by the formula$$(\\nabla f){x}\\cdot v=df{x}(v)$$for any $v \\in \\mathbb{R}^n$. The function $df$, which maps $x$ to $df_x$, is called the differential or exterior derivative of f and is an example of a differential 1-form. 2 Jacobian matrix 雅可比矩阵的重要性在于它体现了一个可微方程与给出点的最优线性逼近。 因此，雅可比矩阵类似于多元函数的导数。 The Jacobian of a vector-valued function in several variables generalizes the gradient of a scalar-valued function in several variables, which in turn generalizes the derivative of a scalar-valued function of a single variable. If $f$ is differentiable at a point $p$ in $\\mathbb{R}^n$, then its differential is represented by $J_f(p)$. In this case, the linear transformation represented by $J_f(p)$ is the best linear approximation of f near the point p, in the sense that$$f ( x ) - f ( p ) = J _ { f } ( p ) ( x - p ) + o ( | x - p | ) \\quad ( \\text { as } x \\rightarrow p )$$This approximation specializes to the approximation of a scalar function of a single variable by its Taylor polynomial of degree one, namely$$f(x)-f(p)=f’(p)(x-p)+o(x-p)\\quad ({\\text{as }}x\\to p)$$The Jacobian matrix represents the differential of $f$ at every point where $f$ is differentiable.$$\\mathbf {J} ={\\begin{bmatrix}{\\dfrac {\\partial \\mathbf {f} }{\\partial x_{1}}}&amp;\\cdots &amp;{\\dfrac {\\partial \\mathbf {f} }{\\partial x_{n}}}\\end{bmatrix}}={\\begin{bmatrix}{\\dfrac {\\partial f_{1}}{\\partial x_{1}}}&amp;\\cdots &amp;{\\dfrac {\\partial f_{1}}{\\partial x_{n}}}\\\\vdots &amp;\\ddots &amp;\\vdots \\{\\dfrac {\\partial f_{m}}{\\partial x_{1}}}&amp;\\cdots &amp;{\\dfrac {\\partial f_{m}}{\\partial x_{n}}}\\end{bmatrix}}$$ 3 Hessian$$\\mathbf {H}(f(\\mathbf {x}))= \\mathbf {J}(\\nabla f(\\mathbf {x}))^T$$ $$\\mathbf {H} ={\\begin{bmatrix}{\\dfrac {\\partial ^{2}f}{\\partial x_{1}^{2}}}&amp;{\\dfrac {\\partial ^{2}f}{\\partial x_{1},\\partial x_{2}}}&amp;\\cdots &amp;{\\dfrac {\\partial ^{2}f}{\\partial x_{1},\\partial x_{n}}}\\[2.2ex]{\\dfrac {\\partial ^{2}f}{\\partial x_{2},\\partial x_{1}}}&amp;{\\dfrac {\\partial ^{2}f}{\\partial x_{2}^{2}}}&amp;\\cdots &amp;{\\dfrac {\\partial ^{2}f}{\\partial x_{2},\\partial x_{n}}}\\[2.2ex]\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots \\[2.2ex]{\\dfrac {\\partial ^{2}f}{\\partial x_{n},\\partial x_{1}}}&amp;{\\dfrac {\\partial ^{2}f}{\\partial x_{n},\\partial x_{2}}}&amp;\\cdots &amp;{\\dfrac {\\partial ^{2}f}{\\partial x_{n}^{2}}}\\end{bmatrix}}$$ 即在每一个变化方向（$\\nabla$）上做线性逼近（Jacobian），这可能可以体现变化程度？","link":"/posts/d99b2e93/"},{"title":"Modeling - Dynamic system modeling intro","text":"https://xlindo.com所有文章遵循CC BY-NC-SA 4.0许可协议。请按许可使用。Blog content follows the CC BY-NC-SA 4.0 License. References 动态系统的建模与分析 DR_CAN bilibili Hung V. Vu, Dynamic Systems: Modeling and Analysis 0 IntroMathematical Modeling and Dynamic Analysis -&gt; Controller Design -&gt; Testing 1 Modeling1.1 Electrical, Electronic, and Electromechanical SystemsKirchhoff’s Current Law (KCL) $i_{in} = i_{out}$ Kirchhoff’s Voltage Law (KVL) $\\sum{e_i}=0$ 6 main elements Voltage, V, e Current, A, i Capacitor, F, C Resistor, Ω, R Inductor, H, L electric charge, C, q Main Relationships $e = i \\cdot R$ $e_L = L \\cdot \\frac{di}{dt}$ $q = C \\cdot e_C$ $e_C = \\frac{1}{C} \\cdot \\int_0^t{i}{dt}$ $i = \\frac{dq}{dt}$ 1.2 Fluid and Thermal SystemsPrecondition $\\rho == const$ Main Elements Flow rate, q, $m^3/s$ Volume, V, $m^3$ Height, h, $m$ Pressure, P, $N/m^3, Pascal$ Hydrostatic Pressure, $P_{Hydro} = \\frac{F_{Hydro}}{A}=\\frac{mg}{A}=\\frac{\\rho gAh}{A}=\\rho gh$ Absolute Pressure, $P_{Abs} = P_a + P_{Hydro}$ Gauge Pressure, $P_{Gauge} = P_{Abs}-P_{Ref}, =\\rho gh$ Fluid Resistence, $R, P_1-P_2 = \\rho qR$ Basic Law: Conservation of Mass $\\frac{dm}{dt}=\\dot{m}{in}-\\dot{m}{out}$ $\\frac{dV}{dt} = q_{in}-q_{out} \\Rightarrow \\frac{dh}{dt} = \\frac{1}{A}(q_{in}-q_{out})$ $\\frac{dh}{dt} = \\frac{1}{A}(q_{in}-\\frac{gh}{R})$ $\\frac{dP}{dt}= \\frac{\\rho g}{A}(q_{in}-q_{out})$ 1.3 Mechanical SystemsMass-Damping-Spring Examples: “System Response” below 2 Analysis2.1 FundamentalsLaplace Transformation Existence of LT: Region of Convergence: $e^{-at}, Re(s)&gt;a$ $s = \\delta + j\\omega$ $F(s)=\\mathcal {L{f(t)}}=\\int _{-\\infty }^{\\infty }e^{-st}f(t),dt$ IFT: $f(t)={\\mathcal {L}}^{-1}{F(t)}$ Differential EquationsTransfer Function $\\frac{X(s)}{U(s)}=G(s)$ e.g., $u(t)=C, \\mathcal L[u(t)] = C\\frac{1}{s}$ 2.2 System Response2.2.1 First-Order Systems Unit-Step Response Time Constant $t=\\tau=\\frac{1}{a}, x(\\tau)=0.63$ Settling Time $T_{ss}=4\\tau=\\frac{4}{a}, x(T_{ss})=0.98$ 2.2.2 Second-Order Systems Unit-Step ResponseMass-Damping-Spring $\\ddot x+\\frac{B}{m}\\dot x+\\frac{K}{m}x=F$ $\\omega_n=\\sqrt{\\frac{K}{m}}$, Natural Frequency $\\zeta = \\frac{B}{2\\sqrt{Km}}$, Damping Ratio 1. Characteristic Equation For response to initial condition: $F=0, x(0)=x_0, \\dot x(0)=\\dot x_0$ $\\ddot x+2\\zeta\\omega_n\\dot x+\\omega_n^2x=0$ with $x(t)=e^{\\lambda t}, \\dot x(t)=\\lambda e^{\\lambda t}, \\ddot x(t)=\\lambda^2e^{\\lambda t}$, obtain characteristic equation and solve and get characteristic roots $\\lambda_{1,2}$ with $\\zeta$ in the expression Situations Overdamped,$\\zeta &gt;1$ Critically damped, $\\zeta =1$ Underdamped, $0&lt;\\zeta &lt;1$ Undamped, $\\zeta =0$ 2. Tranfer Function For Unit-Step Input, $U(s)=\\frac{1}{s}$ $X(s)=U(s)G(s)=\\frac{1}{s}G(s)$ $x(t)=\\mathcal L^{-1}[X(s)]=1-e^{-\\zeta\\omega_n t}\\sqrt{\\frac{1}{1-\\zeta^2}}sin(\\omega_d t+\\phi)$ Focus on underdamped 2.2.3 Assessment $T_d$: Delay time, the time for rise to 50% $T_r$: Rise time, the time for the first arrival to the final value $M_p^%$, Maximum percent overshoot, the vertical distance between the maximum peak $T_{ss}$, settling time, the time required for the response curve to reach and stay within a certain strip(2%$4\\tau$, 5%$3\\tau$) about the final value is referred to as the settling time. $T_p$, peak time 2.2.4 Frequency Response $G(j\\omega)$ Magnitude Response $\\frac{M_o}{M_i}=M$ Phase Response $\\phi_o - \\phi_i = \\phi$ Sinusoidal signal: invariable frequency $u(t)=Asin{\\omega t} + Bcos{\\omega t} \\sim M_i sin(\\omega t+\\phi_i)$ Frequency response actually is steady state response: $x_{ss}(t) = K_1 e^{-j\\omega t}+K_2 e^{j\\omega t}$ $K_1, K_2$ are conjugate $M_G=|G(j\\omega)|$, if in $20log|G(j\\omega)|\\sim \\omega,\\Rightarrow$Bode Plot $\\phi_G=\\angle G(j\\omega)$ Cut-Off Frequency For Low-Pass Filter, $G(s)=\\frac{a}{s+a}, s=j\\omega$: $|G(j\\omega)|=\\sqrt{\\frac{1}{1+(\\frac{\\omega}{a})^2}}$ Cut-Off Freaquency: $\\omega = a, |G(j\\omega)| = 0.707$ For High-Pass Filter, $G(s)=\\frac{s}{s+a}, s=j\\omega$: $|G(j\\omega)|=\\sqrt{\\frac{1}{1+(\\frac{a}{\\omega})^2}}$","link":"/posts/2d662707/"},{"title":"Python-ipdb速查表-Spyder调试","text":"https://xlindo.com licensed under CC BY-NC-SA 4.0 使用Spyder时，默认的调试方式是使用ipdb，ipdb是pdb的增强版，提供了补全、语法高亮等补充内容。本文提供了ipdb的常用参数速查，并在后文中也给出了pdb的常用参数速查。两者大体相同。 1 ipdb 参考：http://wulc.me/2018/12/21/ipdb%20%E4%BD%BF%E7%94%A8%E5%B0%8F%E8%AE%B0/ 1.1 使用 在代码内使用ipdb 123import ipdbipdb.set_trace() 但这种方式会破坏原有代码。 在代码外部使用ipdb。 注意，由于本文目的是使用Spyder下的ipdb，如果不想单独安装ipdb，可以通过自带的pdb完成下述命令测试。 1python -m ipdb &lt;file.py&gt; 1.2 常用命令 &lt;ENTER&gt; 重复上次命令 p(rint) 打印某个变量 c(ontinue) 继续 b(reak) 打断点 cl(ear) 清除断点 n(ext) 让程序运行下一行，如果当前语句有一个函数调用，不会进入被调用的函数体中 s(tep) 跟 n 相似，但是如果当前有一个函数调用，那么 s 会进入被调用的函数体中 l(ist) 显示当前行的上下文 r(eturn) 运行直到子程序结束 !&lt;python 命令&gt; h(elp) 帮助 a(rgs) 打印当前函数的参数 j(ump) 让程序跳转到指定的行数 l(ist) 可以列出当前将要运行的代码块 q(uit) 退出调试 1.3 调试中的帮助调试时，使用 h ，可以查看所有命令。 12345678910111213141516ipdb&gt; hDocumented commands (type help &lt;topic&gt;):========================================EOF cl debug ignore n pp run undisplaya clear disable interact next psource rv unt alias commands display j p q s until args complete down jump pdef quit source up b condition enable l pdoc r step w break cont exit list pfile restart tbreak whatis bt continue h ll pinfo return u where c d help longlist pinfo2 retval unaliasMiscellaneous help topics:==========================exec pdb 使用 h &lt;cmd&gt; 可以查看具体命令的使用。 123ipdb&gt; h pp expression Print the value of the expression. 2 pdb pdb的命令集：https://docs.python.org/3/library/pdb.html出处：https://github.com/nblock/pdb-cheatsheet 2.1 Getting startedstart pdb from within a script: 123import pdbpdb.set_trace() start pdb from the commandline: 1python -m pdb &lt;file.py&gt; 2.2 Basicsh(elp) print available commandsh(elp) command print help about commandq(quit) quit debugger 2.3 Examinep(rint) expr print the value of exprpp expr pretty-print the value of exprw(here) print current position (including stack trace)l(ist) list 11 lines of code around the current linel(ist) first, last list from first to last line numbera(rgs) print the args of the current function 2.4 Movement&lt;ENTER&gt; repeat the last commandn(ext) execute the current statement (step over)s(tep) execute and step into functionr(eturn) continue execution until the current function returnsc(ontinue) continue execution until a breakpoint is encounteredu(p) move one level up in the stack traced(own) move one level down in the stack trace 2.5 Breakpointsb(reak) show all breakpointsb(reak) lineno set a breakpoint at linenob(reak) func set a breakpoint at the first line of a func 2.6 Manipulation!stmt treat stmt as a Python statement instead of a pdb command","link":"/posts/bbc9cd78/"},{"title":"Python-让numpy慢的原因","text":"https://xlindo.com所有文章遵循CC BY-NC-SA 4.0许可协议。请按许可使用。 我们为什么老是觉得python慢，有时候甚至觉得c或者c++慢？其实这些在《深入理解计算机系统》那本书里有答案，之前去面试的时候也被问到过相关的问题。但是，在平时的工作中，却很难驾轻就熟的用起来，直到看到解答的时候才恍然大悟。而我现在的经验就是，看了，记下来，多恍然大悟几次，就会用了。^.^ 最近读了莫烦的一篇文章，觉得很有意义：为什么用 Numpy 还是慢, 你用对了吗? 这里，来梳理一下文章中说让numpy慢的一些原因，或者说一些tricks: 1 row or column?12col_major = np.zeros((10,10), order='C') # C-typerow_major = np.zeros((10,10), order='F') # Fortran 这里以不同的方式创建的np矩阵，在不同的存储方式下对应相同的操作，自然体现出来了不同的速度。 所以，根据自己的数据排列类型选择不同的创建方式，很重要。 2 copy慢，view快这里即是说的view对应了相同的内存，不会产生复制操作。更例如： 12a *= 2 # same as a[:] *= 2,viewb = 2*b # copy 又例如flatten().ravel()，前者需要copy，而后者不需要。 在选择数据时同样涉及到： 12345678910a_view1 = a[1:2, 3:6] # 切片 slicea_view2 = a[:100] # 同上a_view3 = a[::2] # 跳步a_view4 = a.ravel() # 上面提到了a_copy1 = a[[1,4,6], [2,4,6]] # 用 index 选a_copy2 = a[[True, True], [False, True]] # 用 maska_copy3 = a[[1,2], :] # 虽然 1,2 的确连在一起了, 但是他们确实是 copya_copy4 = a[a[1,:] != 0, :] # fancy indexinga_copy5 = a[np.isnan(a), :] # fancy indexing 3 improvement tricks 使用 np.take(), 替代用 index:np.take(a, indices, axis=0) vs. a[indices] 使用 np.compress(), 替代用 mask:12345678mask = a[:, 0] &lt; 0.5def f1(a): for _ in range(N): _ = np.compress(mask, a, axis=0)def f2(b): for _ in range(N): _ = b[mask] 4 out Universal functions a = a + 1慢于a = np.add(a, 1):copy a += 1慢于np.add(a, 1, out=a):out 5 numpy 快于 pandas Numpy Vs Pandas Performance Comparison 因为pandas存了很多相关的数据，速度远慢于numpy.","link":"/posts/dd36c850/"},{"title":"SysId - Input signals in system identification","text":"https://xlindo.com所有文章遵循CC BY-NC-SA 4.0许可协议。请按许可使用。Blog content follows the CC BY-NC-SA 4.0 License. Recently, when I implemented some algorithms in papers, the design of input signals confused me a lot. As for the common design of experiments, it is not that complicated as in Ljung’s book. So I decided to do a easy but systematical study mainly for practice. pdf1pdf2pdf3 PreliminaryThis blog is based on the slide mentioned above. This words below in “Bold“ is what I concern more. Of course I will also do some additions and deletions and my understandings. 0 Main concepts Common input signals $u(t)$ in system identification step function sum of sinusoids ARMA sequences pseudo random binary sequence (PRBS) spectral characteristics persistent excitation 1 Common input signalsMost often the input signal is characterized by its first and second moments: $$\\left{\\begin{array}{l}{m=E[u(t)]} \\ {r(\\tau)=E\\left[(u(t)-m)(u(t)-m)^{T}\\right]}\\end{array}\\right.$$ and/or its spectral density: $$\\phi(\\omega)=\\frac{1}{2 \\pi} \\sum_{\\tau=-\\infty}^{\\infty} r(\\tau) e^{-i \\tau \\omega}$$ 1.1 Step function why step signal? (todo in Unit step response in system identification) In simple terms, the step response includes all information of the control systems. $$u(t)=\\left{\\begin{array}{ll}{0,} &amp; {t&lt;0} \\ {u_{0},} &amp; {t \\geq 0}\\end{array}\\right.$$ where the amplitude $u_{0}$ is arbitrarily chosen Properties Mostly used for transient analysis: overshoot, static gain, major time-constants, rise time. Limited use for parametric modeling. 1.2 Sum of sinusoids why sinusoids? Orthonormal basis. $\\frac{du(t)}{dt}$ and $\\int{u(t)}dt$ both are itself multiplied by a (feature) number. $$u(t)=\\sum_{m=1}^{M} a_{m} \\sin \\left(\\omega_{m} t+\\varphi_{m}\\right)$$ Properties User parameters $a_{m}, \\omega_{m}, \\varphi_{m}$ Covariance function given as$$R(\\tau)=\\sum_{m=1}^{M} \\frac{a_{m}^{2}}{2} \\cos \\left(\\omega_{m} t+\\varphi_{m}\\right)$$ Spectral Density function given as$$\\Phi(\\omega)=\\sum_{m=1}^{M} \\frac{a_{m}^{2}}{2}\\left[\\delta\\left(\\omega-\\omega_{m}\\right)+\\delta\\left(\\omega-\\omega_{m}\\right)\\right]$$ It means that with the spectrum analysis, the different sinusoid inputs will be at the related positions with distinct frequencies. (Fig. Sum of 2 sinusoids. Realization (left), Spectral density (right).) 1.3 Autoregressive Moving Average sequence (ARMA)$$\\frac{1}{N} \\sum_{t=1}^{N} e(t) e(t+\\tau) \\rightarrow 0$$as $N \\rightarrow \\infty$, and where $e(t)$ a pseudorandom sequence similar to white noise, then $u(t)$ can be obatained by linear filtering$$u(t)+c_{1} u(t-1)+\\ldots c_{p} u(t-p)=e(t)+d_{1} e(t-1)+\\ldots+d_{q} e(t-p)$$ $u(t)$ is called ARMA (autoregressive moving average) process when all $c_i = 0$ it is called MA (moving average) process when all $d_i = 0$ it is called AR (autoregressive) process the user gets to choose $c_{i}, d_{i}$ and the random generator of $e(t)$ In general, the transfer function of ARMA(p,q) is $$U(z)=\\frac{D(q)}{C(p)} E(z)$$ where $C(z) =1+c_{1} z^{-1}+c_{2} z^{-2}+\\ldots +c_{p} z^{-p}$ $D(z) =1+d_{1} z^{-1}+d_{2} z^{-2}+\\ldots +d_{q} z^{-q}$ Properties The signal $u(t)$ can be obtained by filtering $e(t)$. different choices of $c_{i}, d_{i}$ lead to inputs with various spectral characteristics. Spectrum of ARMA process let $e(t)$ be a white noise with variance $\\lambda^{2}$, then the spectral density of ARMA process is $$S(\\omega)=\\lambda^{2}\\left|\\frac{D(\\omega)}{C(\\omega)}\\right|^{2}$$ (Fig. ARMA process. Realization (left), Spectral density (right).) 1.4 Pseudorandom Binary Sequence (PRBS)A PRBS $(u(t))t$ is a periodic, deterministic signal with white noise-like properties.$$u(t)=\\operatorname{rem}\\left(A\\left(q^{-1}\\right) e(t), 2\\right)=\\operatorname{rem}\\left(a{1} u(t-1)+\\ldots+a_{n} u(t-n), 2\\right)$$ Properties The signal takes values ${0, 1}$ in a fashion dictated by $A$. Spectral properties are determined by $A(q)$ and in particular by the period length $M = 2^n − 1$. Deterministic sequence behaving as noise (reproducible). (Fig. PRBS signal taking values in {−1, 1}, M = ∞. Realization (left), Spectral density (right).) 2 Persistent excitationa signal $u(t)$ is persistently exciting of order $n$ if the following limit exists: $$R_{u}(\\tau)=\\lim {N \\rightarrow \\infty} \\frac{1}{N} \\sum{t=1}^{N} u(t+\\tau) u^{T}(t)$$ the following matrix is positive definite $$\\mathbf{R}{n}=\\left[\\begin{array}{cccc}{R{u}(0)} &amp; {R_{u}(1)} &amp; {\\dots} &amp; {R_{u}(n-1)} \\ {R_{u}(-1)} &amp; {R_{u}(0)} &amp; {\\cdots} &amp; {R_{u}(n-2)} \\ {\\vdots} &amp; {} &amp; {\\cdots} &amp; {\\vdots} \\ {R_{u}(1-n)} &amp; {R_{u}(2-n)} &amp; {\\cdots} &amp; {R_{u}(0)}\\end{array}\\right]$$ 2.1 Examining the order of persistent excitation2.1.1 white noise input of zero mean and variance $\\lambda^{2}$$$R(\\tau)=\\lambda^{2} \\delta(\\tau), \\quad \\Longrightarrow \\quad \\mathbf{R}{n}=\\lambda^{2} I{n}$$ thus, white noise is persistently exciting of all orders 2.1.2 step input of magnitude $\\lambda$$$R(\\tau)=\\lambda^{2}, \\quad \\forall \\tau \\quad \\Longrightarrow \\quad \\mathbf{R}{n}=\\lambda^{2} \\mathbf{1}{n}$$ so, a step function is persistently exciting of order 1 2.1.3 impulse input: u(t) = 1 for t = 0 and 0 otherwise$$R(\\tau)=0, \\quad \\forall \\tau \\quad \\Longrightarrow \\quad \\mathbf{R}_{n}=0$$ so, an impulse is not persistently exciting of any order 2.2 Some results An input signal is persistently exciting of order $2n$ can be used to consistently estimate parameters of a model of order $\\leq n$. A step function that is persistently exciting of order $1$, but not of greater order. A PRBS with period $M$ is persistently exciting of order $M$, and can generate a $2^M − 1$ full length sequence. An ARMA process is persistently exciting of any finite order. A sum of $M$ sinusoids is persistently exciting of order $n=2M$ (if $\\omega_{m} \\neq 0,-\\pi, \\pi$) A parametric model becomes more accurate in the frequency region where the input signal has a major part of its energy. Summary The choice of input signals determines the quality of the estimate. The estimated model is more accurate in frequency regions where the input signal contains much energy. An input signal has to be ’rich’ enough to excite all interesting modes of the system (PE of sufficiently high order). the choice of input is imposed by the type of identification method some often used signals include PRBS and ARMA processes The choice of a signal input is important on the PE order, that it is larger than the number of parameters. –xlindo","link":"/posts/12e09ac1/"},{"title":"SysId - Prediction with shift operator","text":"https://xlindo.com licensed under CC BY-NC-SA 4.0 Prediction with shift operator Sys. id. Ljung Sec. 3.2 主要想用这一节来理解一下 shift operator 的运算法则。 In this chapter we shall discuss some such uses. The purpose of this is twofold. First, the idea of how to predict future output values will turn out to be most essential for the development of identification methods. Second, by illustrating different uses of system descriptions, we will provide some insights into what is required for such descriptions to be adequate for their intended uses. 3.2 Prediction$$y(t) = G(q)u(t) + H(q)e(t) \\tag{2.97}$$ We shall start by discussing how future values of $v(t)$ can be predicted in case it is described by$$v(t) = H(q) e(t) = \\sum_{k=0}^{\\infty} h(k)e(t-k) \\tag{3.4}$$For (3.4) to be meaningful, we assume that $H$ is stable; that is$$\\sum_{k=0}^{\\infty}|h(k)| &lt; \\infty$$ 3.2.1 Invertibility of the Noise ModelA crucial property of (3.4), which we will impose, is that it should be invertible; that is, if $v(s)$, $s\\leqslant t$, are known, then we shall be able to compute $e(t)$ as$$e(t) = \\tilde H(q) v(t) = \\sum_{k=0}^{\\infty} \\tilde h(k)v(t-k) \\tag{3.6}$$with$$\\sum_{k=0}^{\\infty}|\\tilde h(k)| &lt; \\infty$$ Lemma 3.1. Consider $\\lbrace v(t) \\rbrace$ defined by (3.4) and assume that the filter $H$ is stable. Let$$H(z) = \\sum_{k=0}^{\\infty} h(k)z^{-k} \\tag{3.7}$$and assume that the function $\\frac{1}{H(z)}$ is analytic in $|z| \\geqslant 1$:$$\\frac{1}{H(z)} = \\sum_{k=0}^{\\infty} \\tilde h(k)z^{-k} \\tag{3.8}$$Define the filter $H^{-1}(q)$ by$$H^{-1}(q) = \\sum_{k=0}^{\\infty} \\tilde h(k)q^{-k} \\tag{3.9}$$Then $\\tilde H(q) = H^{-1}(q)$ satisfies (3.6). Remark. That (3.8) exists for $|z| \\geqslant 1$ also means that the filter $H^{-1}(q)$ is stable. For convenience, we shall then say that $H(q)$ is an inversely stable filter. All that is needed is that the function $\\frac{1}{H(z)}$ is analytic in $|z| \\geqslant 1$; that is, it has no poles on or outside the unit circle. We could also phrase the condition as $H(z)$ must have no zeros on or outside the unit circle. 3.2.2 Example 3.1 A Moving Average ProcessSuppose that$$v(t) = e(t) + ce(t-1)$$That is,$$H(q) = 1+cq^{-1}$$For MA(1). Then$$H(z) = 1 + cz^{-1}= \\frac{z+c}{z}$$has a pole in $z=0$ and a zero in $z=-c$, which is inside the unit circle if $|c|&lt;1$. If so, the inverse filter is determined as$$H^{-1}(z) = \\frac{1}{H(z)} = \\frac{1}{1 + cz^{-1}} = \\sum_{k=0}^{\\infty} (-c)^k z^{-k}$$and $e(t)$ is recovered from (3.12) as$$e(t) = \\sum_{k=0}^{\\infty} (-c)^k v(t-k)$$ Calc from definition. Bilateral Z-transform$$X(z)={\\mathcal {Z}}{x[n]}=\\sum _{n=-\\infty }^{\\infty }x[n]z^{-n}$$ Time domain Z-domain Accumulation $\\sum_{k=-\\infty }^{n} x[k] $ ${\\frac {1}{1-z^{-1}}}X(z)$ Proof$$\\begin{aligned}\\sum _{n=-\\infty }^{\\infty }\\sum _{k=-\\infty }^{n}x[k]z^{-n}&amp;=\\sum _{n=-\\infty }^{\\infty }(x[n]+\\cdots +x[-\\infty ])z^{-n}\\&amp;=X[z]\\left(1+z^{-1}+z^{-2}+\\cdots \\right)\\&amp;=X[z]\\sum _{j=0}^{\\infty }z^{-j}\\&amp;=X[z]{\\frac {1}{1-z^{-1}}}\\end{aligned}$$ 3.2.3 One-step-ahead Prediction of $v$$$\\hat v(t|t-1) =\\sum_{k=1}^\\infty h (k)e(t-k) \\tag{3.14}$$ $$\\hat v(t|t-1) =\\sum_{k=1}^\\infty h (k)e(t-k) = [H(q)-1]e(t) \\= \\frac{H(q)-1}{H(q)}v(t) =[1-H^{-1}(q)]v(t)\\=\\sum_{k=1}^\\infty -\\tilde h (k)v(t-k) \\tag{3.15}$$ 3.2.4 Example 3.3 An Autoregressive ProcessConsider a process$$v(t) = \\sum_{k=0}^{\\infty} a^k e(t-k), \\quad |a|&lt;1$$Then$$H(z) = \\sum_{k=0}^{\\infty} a^k z^{-k} = \\frac{1}{1-az^{-1}}$$which gives$$H^{-1}(z) = 1-az^{-1}$$and the predictor, according to (3.15),then$$\\hat v(t|t-1) = a v(t-1)$$ 3.2.5 The Prediction Error$$y(t)- \\hat y(t|t-1) = -H^{-1}(q)G(q)u(t) + H^{-1}(q)y(t) = e(t) \\tag{3.25}$$ The variable $e(t)$ thus represents that part of the output $y(t)$ that cannot be predicted from past data. For this reason it is also called the innovation at time $t$.","link":"/posts/b70ba34f/"},{"title":"TSA-Analysis of the serial correlation","text":"http://xlindo.com所有文章遵循CC BY-NC-SA 4.0许可协议。请按许可使用。Blog content follows the CC BY-NC-SA 4.0 License. In time series analysis, we should always focus on the correlation itself. Because there exists no other series to compare, we define autovariance, autocorrelation function and partial autocorrelation function based on the characteristics of the time series. Word auto means we do the analysis on itself. 1 Autocovariance /wiki/Autocovariance 1.1 IntroductionIn probability theory and statistics, given a stochastic process, the autocovariance is a function that gives the covariance of the process with itself at pairs of time points. Autocovariance is closely related to the autocorrelation of the process in question. It can be thought that the autovariance is the similarity of the signal with with a delayed copy of itself . 1.2 DefinitionWith the usual notation $\\operatorname {E}$ for the expectation operator, if the stochastic process ${X_{t}}$ has the mean function $\\mu {t}=\\operatorname {E} [X{t}]$, then the autocovariance is given by,$$\\begin{aligned}\\mathbf{K}{X X}\\left(t{1}, t_{2}\\right) &amp;= \\operatorname{cov}\\left[X_{t_{1}}, X_{t_{2}}\\right] \\&amp;= \\mathrm{E}\\left[\\left(X_{t_{1}}-\\mu_{t_{1}}\\right)\\left(X_{t_{2}}-\\mu_{t_{2}}\\right)\\right] \\&amp;= \\mathrm{E}\\left[X_{t_{1}} X_{t_{2}}\\right]-\\mu_{t_{1}} \\mu_{t_{2}}\\end{aligned},$$where $t_{1}$and $t_{2}$are two moments in time. Or, we usually use $\\gamma$ as the notion,$$\\gamma(i, j)=E\\left[\\left(X_{i}-\\mu_{i}\\right)\\left(X_{j}-\\mu_{j}\\right)\\right].$$More generally, if the process is a second order stationary process, which means a weakly stationaty process (WSS). From the definition of a weakly stationary signal, the autocovariance and autocorrelation will not depend on $t$.it gives$$\\gamma(k)=\\operatorname{cov}\\left(X_{t+k}, X_{t}\\right)=E\\left[\\left(X_{t}-\\mu\\right)\\left(X_{t+k}-\\mu\\right)\\right],$$where $\\tau$ is the lag time, the time has been shifted. 1.3 calculation$$\\begin{aligned}\\gamma(k)&amp;=\\mathrm{E}\\left[\\left(X_{t}-\\mu_{t}\\right)\\left(X_{t-k}-\\mu_{t}\\right)\\right] \\&amp;=\\mathrm{E}\\left[X_{t} X_{t-k}\\right]-\\mu_{t} \\mu_{t}\\&amp;=\\frac{1}{N-k}\\sum_{t=k+1}^{N}{X_tX_{t-k}}-\\overline{X_t}^2\\end{aligned}$$ 1.4 NormalizationFor a WSS with a variance $\\sigma^2$, it converts to a time-dependent Pearson correlation coefficient,$$\\rho(k)=\\frac{\\gamma(k)}{\\gamma(0)}=\\frac{\\gamma(k)}{\\sigma^{2}}$$where $\\gamma(0)=\\sigma^{2}$. 1.5 MATLAB code for Autocovariance Matrix123456789101112131415161718192021222324252627% autocov_m% Author: xlindo% Date: 2019.7.24% % function: compute the autocovariance of a weakly stationaty process%% INPUT: x, a 1*n series vector% OUTPUT: ac_m, a n*n matrix%function ac_m = autocov_m(x)len_x = length(x);gamma = zeros(1,len_x);mu_x = mean(x);for k=1:len_x-1 gamma(k) = sum(x(1:len_x-k+1).*x(k:len_x)) / (len_x-k)- mu_x^2;endgamma(len_x) = 0;ac_m = zeros(len_x, len_x);ac_m(1,:) = gamma;for i=2:len_x ac_m(i,1:len_x) = [gamma(i:-1:2), gamma(1:len_x-i+1)];end 2 Autocorrelation Function (ACF) /wiki/Autocorrelation 2.1 IntroductionAutocorrelation, also known as serial correlation, is the correlation of a signal with a delayed copy of itself as a function of delay. Informally, it is the similarity between observations as a function of the time lag between them. The analysis of autocorrelation is a mathematical tool for finding repeating patterns, such as the presence of a periodic signal obscured by noise, or identifying the missing fundamental frequency in a signal implied by its harmonic frequencies. It is often used in signal processing for analyzing functions or series of values, such as time domain signals. 2.2 Definitionsee 1.3. Autocorrealtion function is defined by the normalization of the autovariance,$$\\rho(k)=\\frac{\\gamma(k)}{\\gamma(0)}$$ 2.3 Calculation$$\\begin{aligned}\\rho(k)&amp;=\\frac{\\gamma(k)}{\\gamma(0)}\\&amp;=\\sum_{t=k+1}^{n} \\frac{\\left(X_{t}-\\overline{X}\\right)\\left(X_{t-k}-\\overline{X}\\right)}{\\sum_{t=1}^{n}\\left(X_{t}-\\overline{X}\\right)^{2}}\\end{aligned}$$ 2.4 White noiseThe ACF of white noise is Dirac’s function,$$\\rho_{k}=\\delta(k)$$ 2.5 MATLAB codeAs a extension based on the autocovariance, 1acf = autocov_m(p)/var(p); or use autocorr() [acf,lags,bounds] = autocorr(___) additionally returns the lag numbers that MATLAB® uses to compute the ACF, and also returns the approximate upper and lower confidence bounds. 3 Partial Autocorrelation Function (PACF)/wiki/Partial_autocorrelation_function 3.1 IntroductionThe result of ACF actually is not the pure correlation of $X(t)$ and $X(t-k)$, because $X(t)$ will also be influenced by the variables between. In time series analysis, the partial autocorrelation function (PACF) gives the partial correlation of a stationary time series with its own lagged values, regressed the values of the time series at all shorter lags. It contrasts with the autocorrelation function, which does not control for other lags. 3.2 Definition PACF The partial autocorrelation function (PACF) of a stationary process, $x_t$, denoted $\\phi_{h}^{h}\\left(\\text { or } \\phi_{h h}\\right)$, for$h=1,2, \\dots$ is$$\\phi_{11}=\\operatorname{corr}\\left(x_{t+1}, x_{t}\\right)=\\rho(1)$$and$$\\phi_{h h}=\\operatorname{corr}\\left(x_{t+h}-\\hat{x}{t+h}, x{t}-\\hat{x}_{t}\\right), \\quad h \\geq 2$$ 3.3 Calculation$$\\begin{aligned}\\phi(k)&amp;=\\frac{E\\left(X_{t}-E X_{t}\\right)\\left(X_{t-k}-E X_{t-k}\\right)}{\\sqrt{E\\left(X_{t}-E X_{t}\\right)^{2}} \\sqrt{E\\left(X_{t-k}-E X_{t-k}\\right)^{2}}}\\&amp;=\\frac{\\operatorname{cov}\\left[\\left(X_{t}-\\overline{X}{t}\\right),\\left(X{t-k}-\\overline{X}{t-k}\\right)\\right]}{\\sqrt{\\operatorname{var}\\left(X{t}-\\overline{X}{t}\\right) )} \\sqrt{\\operatorname{var}\\left(X{t-k}-\\overline{X}_{t-k}\\right)}}\\end{aligned}$$ 3.4 Matlab codeUse 3.3, e.g. 12phi(4) = cov(p(4:14)-mean(p(4:14)),p(1:11)-mean(p(1:11)) ) / ... (std(p(1:11)-mean(p(1:11)))*std(p(4:14)-mean(p(4:14)))); Or use parcorr() [pacf,lags,bounds] = parcorr(___) additionally returns the lag numbers that MATLAB® uses to compute the PACF, and also returns the approximate upper and lower confidence bounds. 4 Summary Autocovariance= Covariance Autocorrelation = Pearson correaltion coefficient calculated with the expectation of the whole process Partial Autocorrelation = Pearson correaltion coefficient calculated with the expectations respectively 5 ARMA MathWorks Prozess ACF PACF AR($p$) Tails off gradually Cuts off after $p$ lags MA($q$) Cuts off after $q$ lags Tails off gradually ARMA($p, q$) Tails off gradually Tails off gradually","link":"/posts/affea27a/"},{"title":"Win10下Ubuntu(WSL)中Python环境配置笔记","text":"https://xlindo.com所有文章遵循CC BY-NC-SA 4.0许可协议。请按许可使用。 开虚拟机实在太麻烦了，所以准备在Win10的Ubuntu子系统里搭Python环境用，当然我目前也没太清楚Docker会不会更方便，但Win下装Docker仿佛本身就还不可取。 自带的Ubuntu子系统，一来可以方便的用vim写Python小程序运行，二来可以搭一个Jupyter服务器。这样做下来，一切看会起来都会很清爽。 1 Ubuntu 安装这个很简单啦，我直接在应用商店装的Ubuntu 18.04，之前也在设置里面勾选过Windows Subsystem Linux之类的选项，但现在应该不用做那些了？ 装完了就直接进Linux，然后常规操作，更新。 123sudo apt-get updatesudo apt-get upgradesudo reboot 2 Miniconda 安装我也是今天才知道Anaconda之外还有一个精简版本，我觉得对于这种不需要图形的环境实在是太配了，赶紧上了车。所以，我选择安装Miniconda： 123wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.shsudo chmod 777 Miniconda3-latest-Linux-x86_64.sh #懒人操作./Miniconda3-latest-Linux-x86_64.sh 然后顺着一路装下来后，把conda添加到系统路径。 1vim /home/YOURNAME/.bashrc 在尾部添加export PATH=/home/YOURNAME/miniconda3/bin/conda:$PATH,保存退出，刷新，检查一下。这里我只添加了conda命令，也可以直接整个bin目录添加。 12source /home/YOURNAME/.bashrcecho $PATH 3 Python 环境配置Python环境因人而异，这里只配一个基础版。 就个人经验而言，很多机器学习或者数据处理包对Python 3.6 比较友好，所以这里利用conda 创建一个3.6的环境使用。 123conda create -n py36 python=3.6conda activate py36 #conda deactivate 用以退出环境 4 jupyter notebook 配置4.1 Jupyter 安装12conda install jupyterjupyter notebook # 运行jupyter，用给出的链接可在Win10访问 4.2 插件安装 可参考我知道你会用Jupyter Notebook，但这些插件你都会了吗？ Jupyter最佳搭档 Nbextensions 我觉得有这一个插件就够了，里面有很多可选的小项，包括运行时间、TOC什么的很方便的一些功能 1234pip install jupyter_contrib_nbextensionssudo /home/YOURNAME/miniconda3/envs/py36/bin/jupyter contrib nbextension install #因为我没有添加到系统路径jupyter notebook 再运行后就可以看到Nbextensions的选项卡了。 5 vim？到这里，Python的环境就搭好了，对于只用Jupyter 的人来说，到这里也就差不多了，无非自己再conda install 几个包。 对于要在Linux下用vim写程序的人，当然又是Linux环境下的事情了，我比较懒，我可能会选择安装别人做好的一键配置shell去配置vim。","link":"/posts/525d3c8f/"},{"title":"一周的旅行结束，又开始学术啦","text":"3月1号就回家了，然后玩了一个月后又和老婆在欧洲小转了一周，现在终于回到学校考虑毕设的事情了，虽然心里还担心着有没有在自驾的路上违章。 既然又一个人来到这个陌生地，就继续和自己赛跑吧。Flag还是要多立的，哪怕实现不了也是一种激励啊。 其实回想一下这一年在德国的生活，还是能感觉到一种无形的仓促感，一直到现在。虽然一直跟家人说，再过六个月就能回去了，但其实并不知道接下来的事情该怎么去安排。毕设，找工作，回国还是留下，都让人脑瓜疼。这里并没有人管你，所以一切都得靠自己。现在也没收入，对于家里存货的管理也是一头雾水，并不知道应该把自己放到什么消费水平上，但很多东西看了真是想买啊，或许之前在工作的时候随手就买了，但现在是真不敢随便消费。 榨菜也忘了拿，香辣酱被没收了，我这世界胃难道要活生生的搞成德国胃？哈哈，忍一忍就习惯了。当然，花点钱这些钱都能解决，谁让我抠呢，哈哈哈。 今天是媳妇儿生日，和爱因斯坦同一天，两位生日快乐，希望我能沾点智慧之光。","link":"/posts/4e20385c/"},{"title":"三年一瞥","text":"2018.3 - 2021.3 今天是 2021 年 3 月，距离我从计算所离职刚刚满 3 年。 冥冥中仿佛早已注定，昨天在成都偶遇了几个老朋友。约好今晚吃饭，又被反约着蹭了更多的老同事的饭局。 聊技术，聊发展，聊着以前在单位里总被上下级关系所制约的一些有意思的话题。 研究所的生活还是单纯，大家尽心尽力的完成组织上的任务，不用太去考虑经济上的问题。人际关系上简单纯粹。 三年的成长很快，希望在此时的新的三年里能做出来一点有意思的东西。","link":"/posts/7d4bbc2/"},{"title":"备查手册-前馈-反馈控制系统","text":"http://xlindo.com所有文章遵循CC BY-NC-SA 4.0许可协议。请按许可使用。 本文内容梳理自前馈-反馈控制系统的具体分析及其MATLAB／Simulink仿真 0 Motivation 工业控制对象往往伴随大量的干扰，采用常规的反馈控制系统有时很难获得良好的控制质量，为了适应控制系统中存在的这些干扰，在控制理论中提出了补偿控制的基本概念。 单纯的前馈控制是一种开环控制，只能对指定的扰动量进行补偿控制…… 反馈控制系统调节的依据是控制的偏差，它无法将干扰克服在被控变量偏离设定值之前，干扰进入系统后，被控变量产生的波动需经过一段时间才能表现出来，这就限制了反馈控制器作用的充分发挥。 1 前馈控制1.1 控制原理当系统出现扰动时，立刻将其测量出来，通过前馈控制器，根据扰动量的大小来改变控制量，以抵消或减小扰动对被控量的影响。 1.2 原理图 $G_d(s)$ ：干扰通道 $G_f(s)$ ：前馈控制器 $G_1(s)$ ：控制通道 $G_2(s)$ ：被控对象 $M(s)$ ：可测干扰量 1.3 系统输出及补偿条件在扰动量作用下，系统输出为：$$Y(s)=M(s)(G_f(s)G_1(s)+G_d(s))G_2(s)$$ 当$M(s) \\neq 0$时，$Y(s)=0$，表示被控变量在干扰的影响下仍然被稳定控制在期望值上，由此得到如下补偿条件：$$G_f(s)=-\\frac{G_d(s)}{G_1(s)}$$ 2 反馈控制2.1 控制原理将被控量的偏差信号反馈到控制器，由控制器去修正控制量，以减小偏差量。 2.2 原理图 $G_c(s)$：反馈控制器 $G_1(s), G_2(s)$：被控对象 $H(s)$： 反馈通道 2.3 系统输出$$Y(s)=\\frac{X(s)G_c(s)G_1(s)G_2(s)}{1+G_c(s)G_1(s)G_2(s)H(s)}$$ 3 前馈-反馈控制系统3.1 原理图 3.2 系统对扰动的传递函数（前馈）$$\\frac{Y(s)}{M(s)}=\\frac{(G_f(s)G_1(s)+G_d(s))G_2(s)}{1+G_c(s)G_1(s)G_2(s)H(s)}$$ 可见，复合控制与单纯前馈相比： 扰动量对被控量的影响变为原来的$$\\frac{1}{1+G_c(s)G_1(s)G_2(s)H(s)}$$ 前馈补偿条件不变 3.3 系统对输入的传递函数（反馈）$$\\frac{Y(s)}{X(s)}=\\frac{G_c(s)G_1(s)G_2(s)}{1+G_c(s)G_1(s)G_2(s)H(s)}$$ 不影响。 4 控制系统工程整定","link":"/posts/eba9a6eb/"},{"title":"备查手册-卡尔曼滤波器","text":"http://xlindo.com所有文章遵循CC BY-NC-SA 4.0许可协议。请按许可使用。 已收录至https://zhuanlan.zhihu.com/p/52815498本文内容来自官方教程 可以直接跳到6看 0 例 可以列出模型$$\\begin{cases} p_t = p_{t-1}+ v_{t-1} \\times \\Delta t+u_t\\times\\frac{\\Delta t^2}{2} \\ v_t=v_{t-1}+u_t\\times\\Delta t\\end{cases} $$ 则可以写出状态空间方程$$\\left[\\begin{matrix}p_t \\ v_t\\end{matrix}\\right]=\\left[\\begin{matrix}1 &amp; \\Delta t \\ 0 &amp; 1\\end{matrix}\\right]\\left[\\begin{matrix}p_{t-1} \\ v_{t-1}\\end{matrix}\\right]+\\left[\\begin{matrix}\\frac{\\Delta t^2}{2} \\ \\Delta t\\end{matrix}\\right]u_t $$ 1 状态预测公式令$$F_t=\\left[\\begin{matrix}1 &amp; \\Delta t \\ 0 &amp; 1\\end{matrix}\\right],B_t=\\left[\\begin{matrix}\\frac{\\Delta t^2}{2} \\ \\Delta t\\end{matrix}\\right], $$即得出第一个卡尔曼滤波器公式，状态预测公式$$\\displaystyle \\hat{x}t^- = F_t\\hat{x}{t-1}+B_tu_t \\ $$其中$F_t$状态转移矩阵：从上一时刻推测这一时刻；$B_t$为控制矩阵：控制量$u$如何作用；$\\hat{x}_t$为估计值；$\\hat{x}_t^-$为未修正的估计值。 2 不同时刻的不确定性的传递关系公式协方差矩阵 $$\\displaystyle cov(x,x)=\\left[\\begin{matrix}\\sigma_{11} &amp; \\sigma_{12} \\\\sigma_{12} &amp; \\sigma_{22}\\end{matrix}\\right] \\ $$该例中每一个时刻的不确定性都由协方差矩阵$P$来表示，而不确定性的不同时刻传递为 $$\\displaystyle P_t^- = FP_{t-1}^-F^T \\$$再加上预测模型本身的状态转移噪声协方差矩阵 Q ，得到第二个公式，不确定性的传递关系$$\\displaystyle P_t^- = FP_{t-1}^-F^T +Q\\ $$ 3 状态观测 有测量值$z_t$，为一维向量，并加上噪声$v$ ，观测噪声的协方差矩阵为$R$，本例为$R^1$$$\\displaystyle H=[\\begin{matrix}1&amp;0\\end{matrix}] \\ z_t = Hx_t+v\\ $$ 4 状态更新实际观测值与预期观测值的残差乘以$K$作修正$$\\displaystyle \\hat{x}_t = \\hat{x}_t^-+K_t(z_t-H \\hat{x}_t^-)\\ $$其中$K$为卡尔曼系数，$$\\displaystyle K_t = P_t^-H^T(HP_t^-H^T+R)^{-1}\\$$其作用为 权衡预测协方差$P$和观察两协方差$R$： 若相信预测多一点，则残差权重小一点； 若相信观察多一点，则残差权重大一点。 把残差的形式从观察域转换到状态域：$z$为一维向量，而$x$为二维向量，即可以在两个维度同时修正。 5 噪声协方差矩阵的更新更新最佳估计值的噪声分布 $$\\displaystyle P_t = (I-K_tH)P_t^-\\ $$在不确定性的变化中找一种平衡。 6 5个公式预测：$$\\displaystyle \\hat{x}t^- = F_t\\hat{x}{t-1}+B_tu_t \\P_t^- = FP_{t-1}^-F^T +Q\\ $$更新：$$\\displaystyle \\begin{aligned} K_t&amp; = P_t^-H^T(HP_t^-H^T+R)^{-1}\\\\hat{x}_t&amp; = \\hat{x}_t^-+K_t(z_t-H \\hat{x}_t^-)\\ P_t&amp; = (I-K_tH)P_t^- \\end{aligned}\\ $$$x_t$ : t 时刻的状态^: 观测值$^-$: 未修正值$F_t$ : 状态转移矩阵，和具体的线性系统相关$u_t$ : K时刻外界对系统的作用$B$ : 输入控制矩阵，外界的影响如何转化为对状态的影响$P$ : 误差矩阵$Q$ : 预测噪声协方差矩阵$R$ : 测量噪声协方差矩阵$H$ : 观测矩阵$K_t$ : t 时刻的kalman增益$z_t$ : t 时刻的观测值 7 Matlab实例1234567891011121314151617181920212223Z=(1:100); %观测值noise=randn(1,100); %方差为1的高斯噪声Z=Z+noise;X=[0; 0]; %状态P=[1 0; 0 1]; %状态协方差矩阵F=[1 1; 0 1]; %状态转移矩阵Q=[0.0001, 0; 0 0.0001]; %状态转移噪声协方差矩阵H=[1 0]; %观测矩阵R=1; %观测噪声方差figure;hold onfor i=1:100 X_ = F*X; P_ = F*P*F'+Q; K = P_*H'/(H*P_*H'+R); X = X_+K*(Z(i)-H*X_); P = (eye(2)-K*H)*P_; plot(X(1),X(2),'o'); %画点，横轴表示位置，纵轴表示速度endhold off; 结果如下，","link":"/posts/89cdb521/"},{"title":"备查手册-梯度法_共轭梯度法_线搜索","text":"https://xlindo.com所有文章遵循CC BY-NC-SA 4.0许可协议。请按许可使用。 1 问题提出 https://en.wikipedia.org/wiki/Gradient_descent#Python $$\\min_{x} f(x)=x^4-3x^3+2$$ 2 梯度法2.1 梯度下降法 simple gradient method https://en.wikipedia.org/wiki/Gradient_descent#Python 梯度下降法的思想很简单，每次下降方向选择梯度反方向，这样能保证每次值都在减小。$$\\begin{cases} x^{k+1}=x^k+\\alpha d^k\\d^k=-g^k\\end{cases}$$ 但问题是“锯齿”现象，所以效果很差，实际情况中一般不用。 12345678910111213141516171819x_k = 6 # The algorithm starts at x=6alpha = 0.01 # step size multiplierprecision = 0.00001 # end markprevious_step_size = 1 max_iters = 10000 # maximum number of iterationsiters = 0 #iteration counterdf = lambda x: 4 * x**3 - 9 * x**2while previous_step_size &gt; precision and iters &lt; max_iters: prev_x_k = x_k g_k = df(prev_x_k) d_k = -g_k x_k += alpha * d_k previous_step_size = abs(x_k - prev_x_k) iters+=1print(&quot;The local minimum occurs at&quot;, x_k)print(&quot;The number of iterations is&quot;, iters) The local minimum occurs at 2.2499646074278457 The number of iterations is 70 2.2 共轭梯度法 conjugate gradient method https://en.wikipedia.org/wiki/Conjugate_gradient_method 该方法基于梯度下降法。 由于梯度下降法每次下降的方向为负梯度方向，这并不能保证其是最优的方向。通过共轭方向的计算，保证第二步开始的下降方向在一个圆锥内，这能极大的提高下降的效率。 即$$\\begin{cases} x^{k+1}=x^k+\\alpha d^k\\d^k=-g^k+\\beta d^k \\ \\beta = \\frac{g_k^Tg_k}{g_{k-1}^Tg_{k-1}}\\end{cases}$$ 123456789101112131415161718192021222324252627x_k = 6 # The algorithm starts at x=6alpha = 0.01 # step size multiplierprecision = 0.00001 # end markprevious_step_size = 1 max_iters = 10000 # maximum number of iterationsiters = 0 #iteration counterbeta = 0df = lambda x: 4 * x**3 - 9 * x**2g_k = df(x_k)while previous_step_size &gt; precision and iters &lt; max_iters: prev_x_k = x_k prev_g_k = g_k if 0 == iters: d_k = -g_k else: g_k = df(x_k) beta = g_k*g_k/(prev_g_k*prev_g_k) d_k = -g_k + beta*d_k x_k += alpha * d_k previous_step_size = abs(x_k - prev_x_k) iters+=1print(&quot;The local minimum occurs at&quot;, x_k)print(&quot;The number of iterations is&quot;, iters) The local minimum occurs at 2.2500110335395793 The number of iterations is 22 2.3 线搜索 line search 代码与例子来源于https://blog.csdn.net/u014791046/article/details/50831017 线搜加入了对步长的计算。 名为Backtracking Line Search(BLS)的梯度下降法以Armijo-Goldstein条件为方法，在以2.2为基础的搜索方向上选取不越过最优点的最大步长： 定义$0&lt;\\beta&lt;1$，及$0&lt;\\alpha\\leq\\frac{1}{2}$ 从$t=1$开始迭代$t=\\beta t$，计算$$f(x_k-t\\nabla f(x_k)) &gt; f(x_k)-\\alpha_k t|\\nabla f(x_k)|_2^2$$ 其中Armijo条件为$$f(x_k+\\alpha_kd_k) \\leq f(x_k)+\\alpha_k\\beta\\nabla f(x_k)^Td_k, where \\beta \\in (0,1)$$ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576# -*- coding: utf-8 -*-# min 𝑓(𝑥)=(x-3)**2import matplotlib.pyplot as pltdef f(x): '''The function we want to minimize''' return (x-3)**2 def f_grad(x): '''gradient of function f''' return (x-3)*2 x = 6y = f(x)MAX_ITER = 300curve = [y]i = 0step = 0.1previous_step_size = 1.0precision = 1e-4#下面展示的是我之前用的方法，看上去貌似还挺合理的，但是很慢while previous_step_size &gt; precision and i &lt; MAX_ITER: prev_x = x gradient = f_grad(x) x = x - gradient * step new_y = f(x) if new_y &gt; y: #如果出现divergence的迹象，就减小step size step *= 0.8 curve.append(new_y) previous_step_size = abs(x - prev_x) i += 1 print(&quot;The local minimum occurs at&quot;, x)print(&quot;The number of iterations is&quot;, i)plt.figure()plt.show() plt.plot(curve, 'r*-')plt.xlabel('iterations')plt.ylabel('objective function value') #下面展示的是backtracking line search，速度很快x = 6y = f(x)alpha = 0.25beta = 0.8curve2 = [y]i = 0previous_step_size = 1.0precision = 1e-4 while previous_step_size &gt; precision and i &lt; MAX_ITER: prev_x = x gradient = f_grad(x) step = 1.0 while f(x - step * gradient) &gt; f(x) - alpha * step * gradient**2: step *= beta x = x - step * gradient new_y = f(x) curve2.append(new_y) previous_step_size = abs(x - prev_x) i += 1 print(&quot;The local minimum occurs at&quot;, x)print(&quot;The number of iterations is&quot;, i)plt.plot(curve2, 'bo-')plt.legend(['gradient descent', 'BLS'])plt.show() 运行结果如图 The local minimum occurs at 3.0003987683987354 The number of iterations is 40 &lt;Figure size 432x288 with 0 Axes&gt; The local minimum occurs at 3.000008885903001 The number of iterations is 10 3 其他方法其他方法还有如Riccati与Collocation，以后有时间再单独总结。","link":"/posts/976c5057/"},{"title":"备查手册-汉密尔顿函数","text":"https://xlindo.com所有文章遵循CC BY-NC-SA 4.0许可协议。请按许可使用。 已收录至https://zhuanlan.zhihu.com/p/46170280 2018SS 课程 Dynamische Prozessoptimierung 0 最优控制问题 目标函数： $$J = \\min_{u(t)}[\\phi({\\textbf x}(t_f)) + \\int_{t_0}^{t_f}f_0({\\textbf x},{\\textbf u})dt]$$ 控制变量： $$u = [u_{1}(t), u_{2}(t), \\dots ,u_{m}(t)]^{T} $$ 状态变量： $$x = [x_{1}(t), x_{2}(t), \\dots ,x_{n}(t)]^{T} , m \\neq n $$ 状态方程： $$\\dot{\\textbf x} = {\\textbf f}({\\textbf x},{\\textbf u}) $$ 初值： $${\\textbf x}(t_0) ={\\textbf x}_0 $$，已知 终值： $${\\textbf x}(t_f) = {\\textbf x}_f $$，给定或自由 时域： $$t_0 \\leq t \\leq t_f $$，给定或自由 约束条件： $$u_{i,min} \\leq u_i(t) \\leq u_{i,max} ， i=1,\\dots,m \\x_{j,min} \\leq x_j(t) \\leq x_{j,max} ， j=1,\\dots,n $$ 泛函极小值的条件： $$\\delta J[{\\textbf u}^*(t)] = 0 $$ 1 汉密尔顿方法（Hamilton） 显式方程条件下终值无约束的动态优化问题 $$\\min_{\\textbf u(t)} J = \\phi [{\\textbf x}(t_f)] + \\int_{t_0}^{t_f}(f_0({\\textbf x},{\\textbf u},t))dt ，s.t. \\dot {\\textbf x}= {\\textbf f}({\\textbf x},{\\textbf u},t), {\\textbf x}(t_0) = {\\textbf x}_0 $$Hamilton 函数：$$H = f_0( {\\textbf x},{\\textbf u},t) + { \\lambda^T}{\\textbf f}( {\\textbf x}, {\\textbf u}, t) $$其中 $$\\lambda $$为有n个元素的伴随状态向量。 则可将问题改写成无约束问题，目标函数：$$J = \\Phi [x(t_f)] + \\int_{t_0}^{t_f}(H- { \\lambda}^T{\\dot {\\textbf x}})dt $$ 最优值满足以下条件： 状态方程： $$\\dot {\\textbf x} = \\frac {\\partial H} {\\partial{ \\lambda}} $$ 协状态方程： $$\\dot { \\lambda} = -\\frac {\\partial H} {\\partial{\\textbf x}}$$ 极值条件： $$\\frac {\\partial H} {\\partial{\\textbf u}} =0 $$ 边界条件： $$x(t_0) =x_0, \\lambda(t_f) = \\frac {\\partial \\phi} {\\partial{\\textbf x(t_f)}} $$ 1.1 $x_f$ 自由 $$\\min_{\\textbf u(t)} J = \\phi [{\\textbf x}(t_f)] + \\int_{t_0}^{t_f}(f_0({\\textbf x},{\\textbf u},t))dt ， \\dot {\\textbf x}= {\\textbf f}({\\textbf x},{\\textbf u},t)，{\\textbf x}(t_0) = {\\textbf x}_0，{\\textbf g}( {\\textbf x}(t_f),t_f)=0$$ Hamilton 函数： $$H = f_0( {\\textbf x},{\\textbf u},t) + { \\lambda}^T{\\textbf f}( {\\textbf x}, {\\textbf u}, t)$$ 可将问题改写成无约束问题，目标函数： $$J =\\tilde \\phi [{\\textbf x}(t_f)] + \\int_{t_0}^{t_f}(H- { \\lambda}^T{\\dot {\\textbf x}})dt ，$$其中 $$\\tilde \\phi [{\\textbf x}(t_f)] = \\phi [{\\textbf x}(t_f)] + {\\mu}^T{\\textbf g}({\\textbf x}(t_f),t_f) $$ 最优值满足以下条件： 状态方程：$$\\dot{\\textbf x} = {\\textbf f}({\\textbf x},{\\textbf u},t)$$ 协状态方程： $$\\dot { \\lambda} = -\\frac {\\partial H} {\\partial{\\textbf x}} =-\\frac {\\partial f_0} {\\partial{\\textbf x}}-\\frac {\\partial f^T} {\\partial{\\textbf x}}{ \\lambda} $$ 极值条件： $$\\frac {\\partial H} {\\partial{\\textbf u}} =0 $$ 边界条件： $$x(t_0) =x_0, \\lambda(t_f) = \\frac {\\partial \\phi} {\\partial{\\textbf x(t_f)}} + \\frac {\\partial g^T} {\\partial{\\textbf x(t_f)}} {\\mu} $$ 1.2 $t_f$ 自由 $$\\min_{\\textbf u(t),t_f} J = \\phi [{\\textbf x}(t_f)] + \\int_{t_0}^{t_f}(f_0({\\textbf x},{\\textbf u},t))dt ，s.t. \\dot {\\textbf x}= {\\textbf f}({\\textbf x},{\\textbf u},t)，{\\textbf x}(t_0) = {\\textbf x}_0$$ 最优值满足以下条件： 状态方程： $$\\dot{\\textbf x} = {\\textbf f}({\\textbf x},{\\textbf u},t)$$ 协状态方程： $$\\dot { \\lambda} = -\\frac {\\partial H} {\\partial{\\textbf x}} $$ 极值条件： $$\\frac {\\partial H} {\\partial{\\textbf u}} =0 $$ 边界条件： $$x(t_0) =x_0, \\lambda(t_f) = \\frac {\\partial \\phi} {\\partial{\\textbf x(t_f)}} $$ 终值条件Hamilton函数： $$H[{\\textbf x}^*({t_f}^*),{\\textbf u}^*({t_f}^*),{ \\lambda}^*({t_f}^*),{t_f}^*] = -\\frac {\\partial \\phi[{\\textbf x}^*({t_f}^*),{t_f}^*]} {\\partial t_f} $$","link":"/posts/f19225cf/"},{"title":"备查手册-特征值与奇异值","text":"https://xlindo.com所有文章遵循CC BY-NC-SA 4.0许可协议。请按许可使用。 在统计学习方法中，我们会经常使用到特征值和奇异值，本文对其背后的特定意义做一个梳理。 0 我能想到的适用或有益理解的场景 OLS 最小二乘法 PCA分析 数据压缩 Fisher information matrix 矩阵条件数 1 特征值与特征向量 Wikipedia: In linear algebra, an eigenvector or characteristic vector of a linear transformation is a non-zero vector that changes by only a scalar factor when that linear transformation is applied to it. 1.1 定义首先，看这个定义， $$A v=\\lambda v$$ 其中$\\lambda$是特征值(eigenvalue)，$v$为特征向量。 其实这个公式就可以说明很多特点： $A$为(作用)方阵； $v$是$A$的特征向量； $\\lambda$是$A$的特征值，为纯量，表示为对角阵。 让我们根据这个式子展开想象： 矩阵的乘法都是线性变换，式子想说明，特征向量在$A$的作用下进行线性变换，效果是特征向量的$\\lambda$倍伸缩。注意： 并不是所有向量都能被$A$给伸缩，只有$v$中的向量（特征向量）能被其伸缩； 伸缩的尺度$\\lambda$体现$A$的变换能力。 援引维基百科就是“在数学上，特别是线性代数中，对于一个给定的方阵$A$，它的特征向量（eigenvector，也译固有向量或本征向量）$v$经过这个线性变换之后，得到的新向量仍然与原来的$v$保持在同一条直线上，但其长度或方向也许会改变”。这个方向是正负那个方向，不是小旋转。 这样的话，如果我们知道了一个方阵的特征值和特征向量，就知道了这个方阵的线性变换能力。 放到应用场景中就是，我们通过特征值就能掌握当前数据在对应方向上的变换能力。所以某些场景中，我们选取较大的特征值们来代表原数据的变换能力，例如：PCA分析、数据压缩。 1.2 计算那么，怎么得到特征值和特征向量呢？ Matlab：scipy.linalg.eig()提供类似下面Matlab函数的计算。 1234567%matlab[V,D] = eig(A) % returns diagonal matrix D of eigenvalues and matrix V % whose columns are the corresponding right eigenvectors,% so that A*V = V*D. 手算特征值： 除了考试，请使用计算机，写出来仅供娱乐。 $$\\begin{aligned}(\\mathbf{A}-\\lambda \\mathbf{I}) \\mathbf{x} &amp;=0 \\ det(\\mathbf{A}-\\lambda \\mathbf{I}) &amp;=0 \\end{aligned}$$ 2 奇异值和奇异值分解奇异值与上文中的特征值相对应。特征值固然方便使用，但其对原矩阵为方阵的限制为实际情况下所难得的。自然，我们就需要一种更一般化的特征提取方式，这里的特征我们叫奇异值(singular value)，而这种方式正是奇异值分解(singular-value decomposition (SVD))。所以，该分解和结果的意义与特征值类似，但拓展了适用范围。 2.1 定义$$M=U \\Sigma V^{*}$$ $M$： $m\\times n$矩阵； $U$： $m\\times m$ 酉矩阵(unitary matrix)； $\\Sigma$： $m\\times n$ 的对角阵，对角元素非负(奇异值)； $V^{*}$： $n\\times n$ 酉矩阵，是$V$的转置。 需要注意的是，奇异值分解结果并不唯一。 2.2 与特征值分解的关系除开适用范围，我们可以将特征值分解看成SVD的一个特例。而更深层的联系是在于[引自维基百科]，对于$M=U \\Sigma V^{*}$： $U$的列是$M$正交输出基向量，是$MM^T$的特征向量； $\\Sigma$是$M^TM,MM^T$的特征值的非负平方根； $V$的列是$M$正交输入基向量，是$M^T M$的特征向量。 随手验证感受一下： 123456import numpy as npfrom scipy import linalgM = np.array([[1,0,3,1], [1,2,3,2], [1,2,3,4]])U,S,V = linalg.svd(M)print('U=',U,'\\n\\nS=',s,'\\n\\nV=',V.T) U= [[-0.38925572 0.85502491 0.34265491] [-0.56511886 0.07208746 -0.82185404] [-0.7274068 -0.51355215 0.45513025]] S= [4.71147949 1.3423714 ] V= [[-2.27634931e-01 2.17876780e-01 -2.66842116e-02 -9.48683298e-01] [-3.49895643e-01 -4.65155494e-01 -8.13144148e-01 -2.37643771e-16] [-6.82904792e-01 6.53630339e-01 -8.00526349e-02 3.16227766e-01] [-5.99496807e-01 -5.55812428e-01 5.75913294e-01 -1.66533454e-16]] 1linalg.eig(M.dot(M.T)) (array([54.58348639+0.j, 3.60292832+0.j, 0.81358529+0.j]), array([[-0.38925572, -0.85502491, 0.34265491], [-0.56511886, -0.07208746, -0.82185404], [-0.7274068 , 0.51355215, 0.45513025]])) 1S ** 2 array([54.58348639, 3.60292832, 0.81358529]) 1linalg.eig(M.T.dot(M)) (array([5.45834864e+01+0.j, 3.60292832e+00+0.j, 7.98251465e-16+0.j, 8.13585293e-01+0.j]), array([[ 2.27634931e-01, 2.17876780e-01, 9.48683298e-01, -2.66842116e-02], [ 3.49895643e-01, -4.65155494e-01, 2.02012103e-17, -8.13144148e-01], [ 6.82904792e-01, 6.53630339e-01, -3.16227766e-01, -8.00526349e-02], [ 5.99496807e-01, -5.55812428e-01, 1.84126527e-17, 5.75913294e-01]])) 我们从中需要知道的是，这些结果是和$M^TM$或$MM^T$有关系的。 这些性质我在Fisher information matrix中用到了一点点。 2.3 计算 Matlab 1[U,S,V] = svd(A) Python 1U,s,Vh = scipy.linalg.svd(A)","link":"/posts/4c6748ad/"},{"title":"备查手册-独立成分分析 ICA","text":"https://xlindo.com所有文章遵循CC BY-NC-SA 4.0许可协议。请按许可使用。 之前不太清楚这是什么，今天看到《机器学习工程师必知的十大算法》里这一条还没听过用过。而看了个大致介绍，感觉和我的毕设有点契合，所以集中学习一下。 0 参考资料 /wiki/Independent_component_analysis 独立成分分析法基本原理.pdf ICA算法之算法实现 /ica-史上最直白的ICA教程.pdf FastICA on 2D point clouds 百度百科-独立成分分析 1 简介In signal processing, independent component analysis (ICA) is a computational method for separating a multivariate signal into additive subcomponents. This is done by assuming that the subcomponents are non-Gaussian signals and that they are statistically independent from each other. ICA is a special case of blind source separation. 中文不太好的维基：在统计学中，独立成分分析或独立分量分析（Independent components analysis，缩写：ICA） 是一种利用统计原理进行计算的方法。它是一个线性变换。这个变换把数据或信号分离成统计独立的非高斯的信号源的线性组合。独立成分分析是盲信号分离（Blind source separation）的一种特例。 2 定义 一句话，从多通道测量中得到的有若干独立信源线性组合成的观测信号中，将独立成分分解出来。 因为主成分分析只对符合高斯分布的样本点比较有效，所以ICA可以看成是主成分分析与因子分析的延展。 独立成分分析的最重要的假设就是信号源统计独立。这个假设在大多数盲信号分离的情况中符合实际情况。 即使当该假设不满足时，仍然可以用独立成分分析来把观察信号统计独立化，从而进一步分析数据的特性。 独立成分分析的经典问题是“鸡尾酒会问题”（cocktail party problem）。 该问题描述的是给定混合信号，如何分离出鸡尾酒会中同时说话的每个人的独立信号。 An important note to consider is that if $N$ sources are present, at least $N$ observations (e.g. microphones if the observed signal is audio) are needed to recover the original signals. This constitutes the case where the matrix is square ($D=J$ , where $D$ is the number of observed signals and $J$ is the number of source signals hypothesized by the model). Other cases of underdetermined ($D&lt;J$) and overdetermined ($D&gt;J$) have been investigated. 当有$N$个信号源时，通常假设观察信号也有$N$个（例如$N$个麦克风或者录音机）。该假设意味着混合矩阵是个方阵，即$J = D$，其中$D$是输入数据的维数，$J$是系统模型的维数。对于$J &lt; D$和$J &gt; D$的情况，学术界也分别有不同研究。 独立成分分析并不能完全恢复信号源的具体数值，也不能解出信号源的正负符号、信号的级数或者信号的数值范围。 2.1 问题分类Linear independent component analysis can be divided into noiseless and noisy cases, where noiseless ICA is a special case of noisy ICA. Nonlinear ICA should be considered as a separate case. 2.2 General definitionThe data are represented by the observed random vector ${\\boldsymbol {x}}=(x_{1},\\ldots ,x_{m})^{T}$ and the hidden components as the random vector ${\\boldsymbol {s}}=(s_{1},\\ldots ,s_{n})^{T}.$ The task is to transform the observed data using a linear static transformation ${\\boldsymbol {W}}$ as ${\\boldsymbol {s}}={\\boldsymbol {W}}{\\boldsymbol {x}}$, into an observable vector of maximally independent components ${\\boldsymbol {s}}$ measured by some function $F(s_{1},\\ldots ,s_{n})$ of independence. 简单讲，就将观察到的向量通过线性变换，使其本身的隐含成分的独立成分被分离。 2.3 Generative model2.3.1 Linear noiseless ICAThe components $x_{i}$ of the observed random vector ${\\boldsymbol {x}}=(x_{1},\\ldots ,x_{m})^{T}$ are generated as a sum of the independent components $s_{k}$, $k=1,\\ldots ,n$: $$x_{i}=a_{i,1}s_{1}+\\cdots +a_{i,k}s_{k}+\\cdots +a_{i,n}s_{n}$$weighted by the mixing weights $a_{i,k}$. The same generative model can be written in vector form as $\\boldsymbol{x}=\\sum_{k=1}^{n} s_{k} \\boldsymbol{a}{k}$, where the observed random vector ${\\boldsymbol {x}}$ is represented by the basis vectors $\\boldsymbol{a}{k}=\\left(\\boldsymbol{a}{1, k}, \\ldots, \\boldsymbol{a}{m, k}\\right)^{T}$. The basis vectors $\\boldsymbol{a}{k}$ form the columns of the mixing matrix $\\boldsymbol{A}=\\left(\\boldsymbol{a}{1}, \\ldots, \\boldsymbol{a}{n}\\right)$ and the generative formula can be written as $\\boldsymbol{x}=\\boldsymbol{A} \\boldsymbol{s}$, where $\\boldsymbol{s}=\\left(s{1}, \\dots, s_{n}\\right)^{T}$. 2.3.2 Linear noisy ICAWith the added assumption of zero-mean and uncorrelated Gaussian noise $n \\sim N(0, \\operatorname{diag}(\\Sigma))$, the ICA model takes the form $\\boldsymbol{x}=\\boldsymbol{A} \\boldsymbol{s}+\\boldsymbol{n}$. 2.3.3 Nonlinear ICAThe mixing of the sources does not need to be linear. Using a nonlinear mixing function $f( \\cdot| \\theta)$ with parameters $\\theta$ the nonlinear ICA model is $x=f(s | \\theta)+n$. 3 实战操作上的感觉是这样的， 图/百度百科 独立成分分析存在各种不同先验假定下的求解算法。而且，经过ICA处理后，重建的数据幅度可能发生变化，自身也可能翻转。 fastica是sklearn.decomposition下的算法。 Implemented using FastICA: A. Hyvarinen and E. Oja, Independent Component Analysis: Algorithms and Applications, Neural Networks, 13(4-5), 2000, pp. 411-430 前提： 观测数据为数个独立、非高斯信号的线性组合。 此外，白化的意思是，使用白化矩阵消除各观测间的二阶相关性。这可以有效地降低问题的复杂度，而且算法简单，用传统的PCA就可完成。 对于sklearn，我看到了两个包， decomposition.FastICA([n_components, …]): FastICA: a fast algorithm for Independent Component Analysis. decomposition.fastica(X[, n_components, …]): Perform Fast Independent Component Analysis. 其步骤为 下面先看FastICA的示例， 3.1 FastICA on 2D point clouds (sklearn examples) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778# -*- coding: utf-8 -*-print(__doc__)# Authors: Alexandre Gramfort, Gael Varoquaux# License: BSD 3 clauseimport numpy as npimport matplotlib.pyplot as pltfrom sklearn.decomposition import PCA, FastICA# ############################################################################## Generate sample datarng = np.random.RandomState(42)S = rng.standard_t(1.5, size=(20000, 2))S[:, 0] *= 2.# Mix dataA = np.array([[1, 1], [0, 2]]) # Mixing matrixX = np.dot(S, A.T) # Generate observationspca = PCA()S_pca_ = pca.fit(X).transform(X)ica = FastICA(random_state=rng)S_ica_ = ica.fit(X).transform(X) # Estimate the sourcesS_ica_ /= S_ica_.std(axis=0)# ############################################################################## Plot resultsdef plot_samples(S, axis_list=None): plt.scatter(S[:, 0], S[:, 1], s=2, marker='o', zorder=10, color='steelblue', alpha=0.5) if axis_list is not None: colors = ['orange', 'red'] for color, axis in zip(colors, axis_list): axis /= axis.std() x_axis, y_axis = axis # Trick to get legend to work plt.plot(0.1 * x_axis, 0.1 * y_axis, linewidth=2, color=color) plt.quiver(0, 0, x_axis, y_axis, zorder=11, width=0.01, scale=6, color=color) plt.hlines(0, -3, 3) plt.vlines(0, -3, 3) plt.xlim(-3, 3) plt.ylim(-3, 3) plt.xlabel('x') plt.ylabel('y')plt.figure()plt.subplot(2, 2, 1)plot_samples(S / S.std())plt.title('True Independent Sources')axis_list = [pca.components_.T, ica.mixing_]plt.subplot(2, 2, 2)plot_samples(X / np.std(X), axis_list=axis_list)legend = plt.legend(['PCA', 'ICA'], loc='upper right')legend.set_zorder(100)plt.title('Observations')plt.subplot(2, 2, 3)plot_samples(S_pca_ / np.std(S_pca_, axis=0))plt.title('PCA recovered signals')plt.subplot(2, 2, 4)plot_samples(S_ica_ / np.std(S_ica_))plt.title('ICA recovered signals')plt.subplots_adjust(0.09, 0.04, 0.94, 0.94, 0.26, 0.36)plt.show() 3.2 网友的fastICA 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238# -*- encoding:utf-8 -*-import sysimport mathimport numpy as npfrom matplotlib import pyplot as pltfrom sklearn.decomposition import fasticafrom copy import deepcopyclass ICA(): def __init__(self, signalRange, timeRange): # 信号幅度 self.signalRange = signalRange # 时间范围 self.timeRange = timeRange # 固定每个单位时间10个点,产生x self.x = np.arange(0, self.timeRange, 0.1) # 所有的点数 self.pointNumber = self.x.shape[0] # 生成正弦波 def produceSin(self, period=100, drawable=False): y = self.signalRange * np.sin(self.x / period * 2 * np.pi) if drawable: plt.plot(self.x, y) plt.show() return y # 生成方波 def produceRect(self, period=20, drawable=False): y = np.ones(self.pointNumber) * self.signalRange begin = 0 end = self.pointNumber step = period * 10 mid = step // 2 while begin &lt; end: y[begin:mid] = -1 * self.signalRange begin += step mid += step if drawable: plt.plot(self.x, y) plt.show() return y # 生成三角波 def produceAngle(self, period=20, drawable=False): lastPoint = period * 10 - 1 if lastPoint &gt;= self.pointNumber: raise ValueError('You must keep at least one period!') delta = ((-1 - 1) * self.signalRange) / float(self.x[lastPoint] - self.x[0]) y = (self.x[:lastPoint+1] - self.x[0]) * delta + self.signalRange y = np.tile(y, self.pointNumber // y.shape[0])[:self.pointNumber] if drawable: plt.plot(self.x, y) plt.show() return y # 生成uniform噪声 def produceNoise(self, signalRange=None, drawable=False): if signalRange is None: signalRange = self.signalRange y = np.asarray([(np.random.random() - 0.5) * 2 * signalRange for _ in range(self.pointNumber)]) if drawable: plt.plot(self.x, y) plt.show() return y # 混合信号 def mixSignal(self,majorSignal, *noises, **kwargs): mixSig = deepcopy(majorSignal) noiseRange = 0.5 if 'noiseRange' in kwargs and kwargs['noiseRange']: noiseRange = kwargs['noiseRange'] for noise in noises: mixSig += noiseRange * np.random.random() * noise if 'drawable' in kwargs and kwargs['drawable']: plt.plot(self.x, mixSig) plt.show() return mixSig # 让每个信号的样本均值为0,且协方差(各个信号之间)为单位阵 def whiten(self, X): # 加None可以认为是将向量进行转置,但是对于矩阵来说,是在中间插入了一维 X = X - X.mean(-1)[:, None] A = np.dot(X, X.T) D, P = np.linalg.eig(A) D = np.diag(D) D_inv = np.linalg.inv(D) D_half = np.sqrt(D_inv) V = np.dot(D_half, P.T) return np.dot(V, X), V # 就是sklearn的源码里面的logcosh # 源码里有fun_args,用到一个alpha来调整幅度,这里省略没加 # tanh(x)的导数为1-tanh(x)^2 def _tanh(self, x): gx = np.tanh(x) g_x = gx ** 2 g_x -= 1 g_x *= -1 return gx, g_x.mean(-1) def _exp(self, x): exp = np.exp(-(x ** 2) / 2) gx = x * exp g_x = (1 - x ** 2) * exp return gx, g_x.mean(axis=-1) def _cube(self, x): return x ** 3, (3 * x ** 2).mean(axis=-1) # W &lt;- (W_1 * W_1')^(-1/2) * W_1 def decorrelation(self, W): U, S = np.linalg.eigh(np.dot(W, W.T)) U = np.diag(U) U_inv = np.linalg.inv(U) U_half = np.sqrt(U_inv) # rebuild_W = np.dot(np.dot(S * 1. / np.sqrt(U), S.T), W) rebuild_W = np.dot(np.dot(np.dot(S, U_half), S.T), W) return rebuild_W # fastICA def fastICA(self, X, fun='tanh'): n, m = X.shape p = float(m) if fun == 'tanh': g = self._tanh elif fun == 'exp': g = self._exp elif fun == 'cube': g = self._cube else: raise ValueError('The algorighm does not ' 'support the support the user-defined function.' 'You must choose the function in (`tanh`, `exp`, `cube`)') # 不懂, 需要深挖才能知道, sklearn的源码里有这个,查的资料里说是black magic X *= np.sqrt(X.shape[1]) # 随机化W,只要保证非奇异即可,源码里默认使用normal distribution来初始化,对应init_w参数 W = np.ones((n,n), np.float32) for i in range(n): for j in range(i): W[i,j] = np.random.random() # 随机化W的另一种方法,但是这个不保证奇异 # W = np.random.random((n, n)) # W = self.decorrelation(W) # 迭代计算W maxIter = 300 for ii in range(maxIter): gwtx, g_wtx = g(np.dot(W, X)) W1 = self.decorrelation(np.dot(gwtx, X.T) / p - g_wtx[:, None] * W) lim = max(abs(abs(np.diag(np.dot(W1, W.T))) - 1)) W = W1 if lim &lt; 0.00001: break return W # 画图 def draw(self, y, figNum): if y.__class__ == list: m = len(y) n = 0 if m &gt; 0: n = len(y[0]) elif y.__class__ == np.array([]).__class__: m, n = y.shape else: raise ValueError('The first arg you give must be type of list or np.array.') plt.figure(figNum) for i in range(m): plt.subplot(m, 1, i + 1) plt.plot(self.x, y[i]) # 显示 def show(self): plt.show()if __name__ == '__main__': # 设置信号幅度为2,时间范围为[0, 200) ica = ICA(2, 200) # 周期为100的正弦波 gsigSin = ica.produceSin(100, False) # 周期为20的方形波 gsigRect = ica.produceRect(20, False) # 周期为20的三角波 gsigAngle = ica.produceAngle(20, False) # 幅度为0.5的uniform噪声 gsigNoise = ica.produceNoise(0.5, False) # 独立信号S totalSig = [gsigSin, gsigRect, gsigAngle, gsigNoise] # 混合信号X mixSig = [] for i, majorSig in enumerate(totalSig): curSig = ica.mixSignal(majorSig, *(totalSig[:i] + totalSig[i+1:]), drawable=False) mixSig.append(curSig) mixSig = np.asarray(mixSig) # 以下是调用自己写的fastICA, 默认做了白化处理,不用白化效果貌似不太行 xWhiten, V = ica.whiten(mixSig) # fun的选择和你假设的S的概率分布函数有关,一般假设为sigmoid函数, 则对应为tanh W = ica.fastICA(xWhiten, fun='tanh') recoverSig = np.dot(np.dot(W, V), mixSig)# =============================================================================# ica.draw(totalSig, 1)# ica.draw(mixSig, 2)# ============================================================================= ica.draw(recoverSig, 3) ica.show() # 以下是调用sklearn包里面的fastICA # V对应白化处理的变换矩阵即Z = V * X, W对应S = W * Z V, W, S = fastica(mixSig.T) # 不做白化处理的话就不用乘K# =============================================================================# assert ((np.dot(np.dot(W, V), mixSig) - S.T) &lt; 0.00001).all()# =============================================================================# =============================================================================# ica.draw(totalSig, 1)# ica.draw(mixSig, 2)# ============================================================================= ica.draw(S.T, 3) ica.show() 4 应用 optical Imaging of neurons neuronal spike sorting face recognition modelling receptive fields of primary visual neurons predicting stock market prices mobile phone communications color based detection of the ripeness of tomatoes removing artifacts, such as eye blinks, from EEG data. analysis of changes in gene expression over time in single cell RNA-sequencing experiments. studies of the resting state network of the brain.","link":"/posts/ca0caebe/"},{"title":"工具 - Notion, Trello, OneNote等效率工具几句话对比 2019","text":"https://xlindo.com所有文章遵循CC BY-NC-SA 4.0许可协议。请按许可使用。 偶然间，看到各类文章对效率工具的吹捧。又很好奇，其中有些我又确实没有用过，就都试了一试。不如把我测试的感受跟大家分享一下吧！ 我是客观中立的学生党。 0 使用场景我的使用场景是：Android手机，iPad，Windows计算机，三个都要使用，这也是直接扔了印象笔记（免费版支持2个设备）不考虑的原因。 对，所有产品基于免费版本。 1 笔记本之王 OneNote Notability当然是不可错过的好工具，我也一直在用。但是它没有win版本，所以导致我不能直接在win上桌面化操作。 没有什么笔记工具能与OneNote比肩了吧，5个G的免费容量也是够够的了。 平台：Win下我比较喜欢用UWP版本 优点 全面、均衡的好 大厂出品，前景美好 缺点 原生不支持Markdown不知道算不算一个小遗憾，因为对于我，我比较喜欢写md 2 模板笔记大师 Notion各类平台无脑吹的最多的一款看似“全能”的笔记应用。花里胡哨。 我把他留着使用的原因是他丰富的模板，当我要在OneNote里写类似笔记时，我会去参考一下它的模板内容。 我一直不太推荐这种所谓的“全能型”工具，除非你是苹果、微软这种有强大的各方面能力支撑的公司，不然可能还是专心干一件事会比较好。 优点 模板丰富，丰富到感动你 可选block 插件丰富，可以做出漂亮的、一体的笔记 缺点 慢。无形之中的慢，总感觉卡卡的 花里胡哨，到处都是block和操作按钮。有一种富文本之于markdown的感觉 免费版1000 blocks限制 被墙风险，毕竟可以分享，有不可控风险 3 行事规划 Trello，Teambition3.1 Trello这一波试用后，留在我电脑上的应用。推荐。 这是一个逻辑简单的项目规划类应用，而我将其作为我的个人规划应用。我最近用它来做找工作和写毕设的规划。 如果你和我一样以前没用过，来看一看界面就知道这软件是怎么回事了。 优点 界面美观 模板，尤其是个人项目逻辑友好 缺点 免费版的Power-Ups ，也就是原生插件，只支持一个，但是够用了 被墙风险，不够这类应用也不用担心这个过多，墙了就转回Teambition 3.2 国产计划行事 TeambitionTrello类似的应用，国内用户推荐，国外用户不推荐。 优点 国产，被墙风险小 本地支持 缺点 国内服务器对国外用户有点慢，账号又和国际服务器不通，导致随时登出，然后报个json的错误出来。就卸载了 手机上操作逻辑我不太喜欢。为什么一屏就显示一个card？ 4 大纲笔记：Dynalist，幕布4.1 DynalistMarkdown用户的笔记应用首选，比较完美的一款产品。需要搭配一个图床，我一直在用。 优点 华人产品 缺点 不支持代码块，这个要加上了就完美了 文档多了后，树状结构看起来很乱 4.2 幕布总体逻辑和Dynalist一致，有其独特优点。 优点 转换笔记到脑图 缺点 不支持Latex，不支持Latex公式必然是个巨大的败笔 5 定时任务：To-Do，奇妙清单5.1 To-Do奇妙清单的微软版。无脑吹微软的产品，大厂的软件生命值得期待、值得信赖。 优点 清爽，免费，前景好 和微软产品的逐渐耦合 缺点 应该都是暂时的，比如iPad还没有Widget 5.2 奇妙清单毫无疑问，奇妙清单在这类应用中几乎无懈可击。但从商业角度上看，我认为To-Do会逐渐吸收奇妙清单的优点，并被微软优先支持。 优点 完美 缺点 被大厂收购了，前景没有To-Do好 6 计算器 Uno Calculator彩蛋推荐。 该计算器是基于Windows默认计算器的开源代码创建，Android和iPad都有下载。好用，四舍五入的看成是微软出品吧。 最后，欢迎读者指正，也期待您给我推荐更好用的工具。","link":"/posts/b5919051/"},{"title":"性能测试-Armadillo(OpenBLAS), Eigen3, numpy, QR分解","text":"https://xlindo.com licensed under CC BY-NC-SA 4.0 想一窥两个矩阵库的性能，写了个程序，对比测试了下两个库在 QR 分解上的计算时间。 为了不让错误的结论影响他人，诚邀勘误。 声明：请勿将此文作为严肃文章参考！Notice: This article is not rigorous! 1 环境 某不方便透露的比较强的服务器。 GCC 7.5.0 CentOS 6 2 版本 numpy，1.19.2 Armadillo 10.7.3，动态库 OpenBLAS 0.3.18，动态库 Eigen 3.4.0，头文件 3 数据4 个随机数矩阵 12345678import numpy as npnp.random.seed(30)m10 = np.random.randint(10000, size=(10, 10))m100 = np.random.randint(10000, size=(100, 100))m1000 = np.random.randint(10000, size=(1000, 1000))m10000 = np.random.randint(10000, size=(10000, 10000)) 4 代码 加载数据 numpy，使用 loadtxt Armadillo，使用 arma::mat 和它的 load() 函数 Eigen3，使用 fstream 写入 Eigen::MatrixXd QR 分解 Armadillo，使用 arma::qr_econ() 计算后取得 R 矩阵 Eigen3，使用 HouseholderQR&lt;MatrixXd&gt; 构造初始化后，由 matrixQR().triangularView&lt;Upper&gt;() 取得 R 矩阵 计时 numpy，使用 %%timeit Armadillo，使用 std::chrono 计时，各跑 5 次后计算平均数 Eigen3， 使用 std::chrono 计时，各跑 5 次后计算平均数 5 结果 QR 分解平均时间，单位（除特别说明外， )，四舍五入到个位 R 矩阵末行末列数，作为判断计算结果正确性依据 numpy 10x10，846 µs ± 793 µs 100x100，3.96 ms ± 2.93 ms 1000x1000，60.1 ms ± 3.68 ms 10000x10000，6.78 s ± 7.94 ms Armadillo 10x10，95 100x100，11 867 1000x1000，8 451 469 10000x10000，_154254133_；（计算错误） Eigen3 10x10，145 100x100，17 508 1000x1000，7 898 760 10000x10000，7 402 371 332 Eigen3，采取 -O2 编译选项 10x10，10 100x100，402 1000x1000，118 003 10000x10000，119 083 247 Eigen3，采取 -O2 -march=native 编译选项感谢知乎评论区 @插地魔 指导 10x10，15 100x100，316 1000x1000，35 439 10000x10000，31 657 171 6 结论 numpy 快，不知具体原因（可能 C++ 这边受运行时库拖累了？） Armadillo 在矩阵较小时，对 Eigen3 有微弱优势；但当矩阵较大时，不能得出结果 Eigen3 仅使用头文件就能运行；能在矩阵较大时得出正确结果；采取 -O2 编译选项后，Eigen3 速度得到大幅提升，但仍比 numpy 慢很多。 Eigen3，棒！","link":"/posts/ecb42a36/"},{"title":"闲","text":"","link":"/posts/45375/"},{"title":"非线性系统-术语表","text":"https://xlindo.com所有文章遵循CC BY-NC-SA 4.0许可协议。请按许可使用。 摘自《非线性系统（第三版）》中文版 A Accumulation 聚点 Adaptive control 自适应控制 Algebraic multiplicity 代数重数 Almost-periodic oscillation 殆周期振荡 Approximate solution 近似解 Asymptotic method 渐近分析法 Asymptotic stability 渐近稳定性 Asymptotically stable equilibrium point 渐进稳定平衡点 Augmented equation 增广方程 Augmented system 增广系统 Automotive suspension 自动悬置 Autonomous system 自治系统 Averaging method 平均化法 Averaging system 平均系统 Averaging 平均化 B Backlash 回差 Backstepping 反步 Banach Space Banach空间 Bias term 偏项 Bifurcation 分岔 Biochemical reactor 生化反应器 Bistable circuit 双稳态电路 Block modal form 块模式 Block triangular 块三角阵 Boundary layer 边界层 Boundary point 边界点 Boundary-layer interval 边界层区间 Boundary-layer sustem（ model）边界层系统（模型） Bounded in the mean 均值有界 Bounded input-bounded output stability 有界输入-有界输出稳定性 Boundedness 有界性 C Canonical form 标准形 Cascade system 级联系统 Causal mapping 因果映射 Center 中心 Center manifold theorem 中心流形定理 Chaos 混沌 Chattering 抖动 Circle criterion 圆判据 Class K function K类函数 Class KL function KL类函数 Closed disk 闭圆盘 Closed orbit 闭轨道 Closed set 闭集 Closed-form expression 闭式表达式 Closed-loop system 闭环系统 Closeness solutions 解的封闭性 Closure of set 集合的闭包 Compact positively invariant subset 正不变紧子集 Compact set 紧集 Compactness 紧性 Comparison method 比较法 Comparison principle lemma） 比较引理（方法） Complete integrability 完全可积性 Complete 完备的 Composite lyapunov function Lyapunov函数 Comprehensive coverage 全收敛 Connectively asympotically stable 互联渐近稳定 Conservative system 保守系统 Constant signal 恒定信号 Continuity of solution 解的连续性 Continuum 连续统 Contours 周线 Contraction mapping theorem 压缩映射定理 Critical circle 临界圆 Critical clearance time 临界清除时间 Cross-product 向量积项 D Damping injection 阻尼注入 DC motor 直流电机 Field-controlled DC motor 场控直流电机 Dead-zone 死区 Decoupled equation 去耦方程 Decrescent function 递减函数 Describing function method 描述函数法 Detectability 可检测 Diagonally dominant matrix 对角占优矩阵 Diffeomorphism 微分同胚 Domain 定义域 Dominant term 主项 Dynamical system 动力学系统 E Energy-like function 类能量函数 Equilibrium point 平衡点 Equivalence of norms 等价范数 Equivalent control 等效控制 Euclidean norm 欧几里得范数 Exact solution 精确解 Exponential stability 指数稳定性 Extended space 扩展空间 F Feedback linearization 反馈线性化 Feedback passivation 反馈无源 Finite escape time 有限逃逸时间 Finite set 有限集 Fixed point 不动点 Foliations 叶状结构 Frozen parameters 冻结参数 Frozen value 冻结值 Full-state-linearization 全状态线性化 G Gain-scheduled controller 增益分配控制器 General form 通式 Global stabilization 全局稳定 Gradient system 梯度系统 Gradient vector 梯度向量 H $$H_\\infty$$control $$H_{ \\infty }$$控制 $$H_{\\infty }$$ norm $ H_{\\infty } $ 范数 Hardening spring 硬化弹簧 Harmonic balance 谐振平衡 High-gain feedback 高增益反馈 High-gain observer 高增益观测器 Homoclinic orbit 同宿轨道 Homotopic invariance 同伦不变 Hywteresiss 迟滞 I Identification experiments 辨识实验 Implicit function theorem 隐函数定理 Induced matrix nom 导出阵模 Infimum下确界 Inner product 内积 Input-output stability 输入-输出稳定性 Input-state linearization 输入状态线性化 Input strictly passive 输入严格无源 Input-output linearization 输入输出线性化 Input-to-state stability 输入状态稳定性 Integral control 积分控制 Integrator backstepping 积分器反步 Interconnected systems 互联系统 Internal model principle 内模原理 Intrinsic property 内蕴性质 Invariance principle 不变原理 Invariance-like theorems 类不变定理 Invariant distribution 对合分布 Inariant set 不变集 Inverse map 逆映射 Inverted pendulum 倒摆 ISS 输入-状态稳定性 J Jump phenomena 跳跃现象 Jacobi matrix Jacobi 矩阵 Josephson juction Josephson结 K Kalman-Yakubovich-Popov lemma Kalman-Yakubovich-Popovch-引理 Kinetic energy 动能 Krasovskii’s method Krasovskii方法 L $$L_{2 }$$ gain $$L_{2}$$ 增益 $$L_{p}$$ space $$L_{p}$$ 空间 Lasalle’s theorem Lasalle定理 Leading principal minors 前主子式 Lie brackets Lie括号 Lie derivative Lie导数 Limit circle 极限环 Limit set 极限集 Linear growth bound 线性增长界 Linear growth condition 线性增长条件 Linear vector space 线性向量空间 linearization 线性化 Lipschitz condition Lipschitz条件 Locus轨迹 Loop transformation 环路变换 Lossless system 无损耗系统 Lyapunov function Lyapunov函数 Lyapunov redesign Lyapunov再设计 Lyapunov stability Lyapunov稳定性 Lyapunov surface Lyapunov曲面 Lyapunov’ s indirect method Lyapunov间接法 Lyapunov’s stability theorem Lyapunov稳定性定理 M M-matrix M矩阵 Magnetic suspension system 磁悬浮系统 Magnitude notion 量值记号 Manifold 流形 Map 映射 Mass-spring system 质量弹簧系统 Matching condition 匹配条件 Mean value theorem 均值定理 Measures 测度 Memoryless function 无记忆函数 Minimal realization 最小实现 Minimum phase system 最小相位系统 Min-max control 最小最大控制 Model reference adaptive control system 参考模型自适应控制系统 Monotonically increasing (decreasing) 单调递增（递减） Moving average 移动平均 Mulitple isolated equilibria 多孤立平衡点 Multiple-time-scale 多时间尺度 N Negative definite function 负定函数 Negative resistance oscillator 负阻振荡器 Negative semidefinite function 半负定函数 Neighborhood 邻域 Neural network神经网络 Nominal model 标称模型 Nominal system 标称系统 Nominal value 标称值 Non vanishing perturbation 非零扰动 Nonautonomous system 非自治系统 Nonempty, compact,invariant set 非空不变集 Nonlinear damping 非线性阻尼 Nonlinearity 非线性 Nontrivial null space 非平凡零空间 Nontrivial periodic solution 非平凡周期解 Normal form 标准形 Normal rank 正常秩 Normed linear space 赋范线性空间 Norm 范数 O Observability 可观测性 Observer-based controller 基于观测器的控制器 One-degree freedom 一阶自由度 Open and connected set 开连通集 Orbit 轨道 Output strictly passive 严格输出无源 P Parasitic parameters 寄生参数 Passive system 无源系统 Passivity-based control 基于无源性的控制 Peaking phenomenon 峰化现象 Periodic orbit 周期轨道 Periodic solution 周期解 Persistently exciting 持续激励 Perturbation method 扰动法 Phase plane 相平面 Phase portrait 相位图 Phase-locked loop 锁相环 PI(Propotional-Integal) 比例积分 PID(Propotional-Integral-Derivative) 比例积分微分 Piecewise linear analysis 分段线性分析 Plant 设备（被控对象） P-norm P-范数 Popov plot Popov曲线 Positive definite function 正定函数 Positive limit point 正极限点（集） Positive semidefinite function 半正定函数 Potential energy 势能 Prey-predatoryey-system 捕食系统 Proper map 正则映射 Q Quadratic form 二次型 Quasi-steady-state model 准稳态模型 R Radially unbounded function 径向无界函数 Reaching phase 到达相位 Recursive procedure 递归过程 Reduced system (model) 降阶系统（模型） Region of attraction 吸引区 Relative degree 相对阶 Relaxation oscillation 松弛振荡 Relaxes monotonically 单调释放 Robust control 鲁棒控制 Robustness 鲁棒性 Rotating rigid body 刚性旋转体 S Saddle point鞍点 Saturation function 饱和函数 Scheduling variable 分配变量 Sector condition 扇形区域条件 Semiglobal stabilization 半全局稳定 Separation principle 分离原理 Separatrix 分界线 Settling time 稳定时间 Sigmoid function S形函数 Signum function 符号函数 Similarity transformation 相似变换 Simple order of magnitude bound 单阶量界 Simply connected region 简单连通域 Singular perturbation method 奇异扰动方法 Skew-symmetric 斜称的 Sliding manifold滑动流形 Sliding mode control 滑模控制 Sliding phase 滑动相位 Slow model 慢模型 Softening spring 软化弹簧 Standard singular perturbation model 标准奇异扰动模型 State equation状态方程 State feedback control 状态反馈控制 State model 状态模型 State plane 状态平面 Steady-state control 稳态控制 Storage function 存储函数 Strength of the interconnections 互联强度 Strictly proper transfer function 严格正则传递函数 Structured perturbation 结构扰动 Subharmonic oscillation 分频振荡 Supremum 上确界 T Time-invariant linear system 线性时不变系统 Time-variant linear system 线性时变系统 Torque 力矩 Tracking 跟踪 Trajectory 轨线 Transfer function 传递函数 Transient response 瞬态响应 Transition function 传递函数 Triangle inequality 三角不等式 True constant parameter 真常数 Truncation 舍位 Tunnel diode circuit 隧道二极管电路 Two-time-scale 二时间尺度 U Ultimate bound 最终边界 Ultimate boundedness 毕竟有界性 Underlying norm 底范数 Unextended space 未扩展空间 Uniform bound 一致有界 Uniformly asymptotically stable 一致渐近稳定 Uniformly continuous 一致连续 Unmodeled dynamics system 未建模动力学系统 Unstructured perturbation 非结构扰动 Upper bound 上界 Upper right-hand derivative 上右导数 VVariable gradient method 可变梯度法Variable structure control 可变结构控制Vector field diagram 向量场图Vector space 向量空间 WWien-bridge oscillator 文氏桥振荡器Worst case analysis 最坏情况分析 Z Zero dynamics 零动态 Zero-observability 零状态可观测性","link":"/posts/b0209132/"}],"tags":[{"name":"生活","slug":"生活","link":"/tags/%E7%94%9F%E6%B4%BB/"},{"name":"随笔","slug":"随笔","link":"/tags/%E9%9A%8F%E7%AC%94/"},{"name":"随笔","slug":"随笔","link":"/tags/%E9%9A%8F%E7%AC%94/"},{"name":"c-cpp","slug":"c-cpp","link":"/tags/c-cpp/"},{"name":"编程","slug":"编程","link":"/tags/%E7%BC%96%E7%A8%8B/"},{"name":"数据结构与算法","slug":"数据结构与算法","link":"/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"leetcode","slug":"leetcode","link":"/tags/leetcode/"},{"name":"程序","slug":"程序","link":"/tags/%E7%A8%8B%E5%BA%8F/"},{"name":"about","slug":"about","link":"/tags/about/"},{"name":"MATLAB","slug":"MATLAB","link":"/tags/MATLAB/"},{"name":"控制","slug":"控制","link":"/tags/%E6%8E%A7%E5%88%B6/"},{"name":"inEnglish","slug":"inEnglish","link":"/tags/inEnglish/"},{"name":"kNN","slug":"kNN","link":"/tags/kNN/"},{"name":"k近邻","slug":"k近邻","link":"/tags/k%E8%BF%91%E9%82%BB/"},{"name":"算法","slug":"算法","link":"/tags/%E7%AE%97%E6%B3%95/"},{"name":"数据分析","slug":"数据分析","link":"/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"name":"线性代数","slug":"线性代数","link":"/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"},{"name":"系统辨识","slug":"系统辨识","link":"/tags/%E7%B3%BB%E7%BB%9F%E8%BE%A8%E8%AF%86/"},{"name":"建模","slug":"建模","link":"/tags/%E5%BB%BA%E6%A8%A1/"},{"name":"笔记","slug":"笔记","link":"/tags/%E7%AC%94%E8%AE%B0/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"numpy","slug":"numpy","link":"/tags/numpy/"},{"name":"TSA","slug":"TSA","link":"/tags/TSA/"},{"name":"经验","slug":"经验","link":"/tags/%E7%BB%8F%E9%AA%8C/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"备查手册","slug":"备查手册","link":"/tags/%E5%A4%87%E6%9F%A5%E6%89%8B%E5%86%8C/"},{"name":"最优化","slug":"最优化","link":"/tags/%E6%9C%80%E4%BC%98%E5%8C%96/"},{"name":"工具","slug":"工具","link":"/tags/%E5%B7%A5%E5%85%B7/"},{"name":"机器学习了吗","slug":"机器学习了吗","link":"/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BA%86%E5%90%97/"},{"name":"性能测试","slug":"性能测试","link":"/tags/%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"name":"非线性系统","slug":"非线性系统","link":"/tags/%E9%9D%9E%E7%BA%BF%E6%80%A7%E7%B3%BB%E7%BB%9F/"}],"categories":[{"name":"巡游记","slug":"巡游记","link":"/categories/%E5%B7%A1%E6%B8%B8%E8%AE%B0/"},{"name":"编程之路","slug":"编程之路","link":"/categories/%E7%BC%96%E7%A8%8B%E4%B9%8B%E8%B7%AF/"},{"name":"System Identification","slug":"System-Identification","link":"/categories/System-Identification/"},{"name":"机器学习","slug":"机器学习","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"数学与统计","slug":"数学与统计","link":"/categories/%E6%95%B0%E5%AD%A6%E4%B8%8E%E7%BB%9F%E8%AE%A1/"},{"name":"经验之谈","slug":"经验之谈","link":"/categories/%E7%BB%8F%E9%AA%8C%E4%B9%8B%E8%B0%88/"},{"name":"控制理论","slug":"控制理论","link":"/categories/%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA/"}],"pages":[{"title":"404","text":"公益404页面 // xlindo","link":"/404.html"},{"title":"About Xlindo","text":"Be pure, be cool. EducationMaster of Science in Technical Cybernetics and Systems Theory. Topics of Current Interest AI for EDA AI acceleration AI applications ContactClick and mail to me: hi@xlindo.com","link":"/about/index.html"}]}